{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from dataclasses import InitVar, dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Iterable, NamedTuple, Sequence, TypeAlias\n",
    "\n",
    "from lightning import pytorch as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit.Chem import Mol\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from chemprop.data.collate import BatchMolGraph, collate_batch\n",
    "from chemprop.data.datapoints import MoleculeDatapoint\n",
    "from chemprop.data.datasets import Datum, MoleculeDataset, MulticomponentDataset, ReactionDataset\n",
    "from chemprop.data.molgraph import MolGraph\n",
    "from chemprop.data.splitting import make_split_indices, split_data_by_indices\n",
    "from chemprop.featurizers import Featurizer, SimpleMoleculeMolGraphFeaturizer\n",
    "from chemprop.models import MulticomponentMPNN, multi\n",
    "from chemprop.nn.agg import Aggregation, MeanAggregation\n",
    "from chemprop.nn.hparams import HasHParams\n",
    "from chemprop.nn.message_passing import BondMessagePassing, MulticomponentMessagePassing, MessagePassing\n",
    "from chemprop.nn.metrics import ChempropMetric\n",
    "from chemprop.nn.predictors import Predictor, RegressionFFN\n",
    "from chemprop.nn.transforms import ScaleTransform, UnscaleTransform\n",
    "from chemprop.nn.utils import Activation, get_activation_function\n",
    "from lightning.pytorch.core.mixins import HyperparametersMixin\n",
    "from chemprop.conf import DEFAULT_ATOM_FDIM, DEFAULT_BOND_FDIM, DEFAULT_HIDDEN_DIM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extend the `MolGraph` class to include a global attribute `w_fp` which is the weight of the learned fingerprint of the molecule when averaging the fingerprints of components in the mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also chemprop.data.molgraph.MolGraph\n",
    "class ComponentMolGraph(NamedTuple):\n",
    "    V: np.ndarray\n",
    "    E: np.ndarray\n",
    "    edge_index: np.ndarray\n",
    "    rev_edge_index: np.ndarray\n",
    "    w_fp: float = 1.0\n",
    "    \"\"\"the weight of the component's fingerprint when combining components in the mixture\"\"\"\n",
    "\n",
    "\n",
    "# See also chemprop.data.datasets.Datum\n",
    "class ComponentDatum(NamedTuple):\n",
    "    mg: ComponentMolGraph\n",
    "    V_d: np.ndarray | None\n",
    "    x_d: np.ndarray | None\n",
    "    y: np.ndarray | None\n",
    "    weight: float\n",
    "    lt_mask: np.ndarray | None\n",
    "    gt_mask: np.ndarray | None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batched versions of `MolGraph` and `Datum` are created during collating datapoints. These are also extended, as well as the entire batch representing all components in the mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also chemprop.data.collate.BatchMolGraph\n",
    "@dataclass(repr=False, eq=False, slots=True)\n",
    "class BatchComponentMolGraph(BatchMolGraph):\n",
    "    mgs: InitVar[Sequence[ComponentMolGraph]]\n",
    "    w_fps: Tensor = field(init=False)\n",
    "\n",
    "    def __post_init__(self, mgs):\n",
    "        super(BatchComponentMolGraph, self).__post_init__(mgs)\n",
    "        self.w_fps = torch.from_numpy(np.array([mg.w_fp for mg in mgs])).float()\n",
    "\n",
    "    def to(self, device: str | torch.device):\n",
    "        super(BatchComponentMolGraph, self).to(device)\n",
    "        self.w_fps = self.w_fps.to(device)\n",
    "\n",
    "\n",
    "# See also chemprop.data.collate.TrainingBatch\n",
    "class BatchComponentDatum(NamedTuple):\n",
    "    bmg: BatchComponentMolGraph\n",
    "    V_d: Tensor | None\n",
    "    X_d: Tensor | None\n",
    "    Y: Tensor | None\n",
    "    w: Tensor\n",
    "    lt_mask: Tensor | None\n",
    "    gt_mask: Tensor | None\n",
    "\n",
    "\n",
    "# See also chemprop.data.collate.MulticomponentTrainingBatch\n",
    "class MixtureBatch(NamedTuple):\n",
    "    bmgs: list[BatchMolGraph | BatchComponentMolGraph]\n",
    "    V_ds: list[Tensor | None]\n",
    "    X_d: Tensor | None\n",
    "    Y: Tensor | None\n",
    "    w: Tensor\n",
    "    lt_mask: Tensor | None\n",
    "    gt_mask: Tensor | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also chemprop.data.collate.collate_batch\n",
    "def collate_component(batch: Iterable[Datum]) -> BatchComponentDatum:\n",
    "    mgs, V_ds, x_ds, ys, weights, lt_masks, gt_masks = zip(*batch)\n",
    "\n",
    "    return BatchComponentDatum(\n",
    "        BatchComponentMolGraph(mgs),\n",
    "        None if V_ds[0] is None else torch.from_numpy(np.concatenate(V_ds)).float(),\n",
    "        None if x_ds[0] is None else torch.from_numpy(np.array(x_ds)).float(),\n",
    "        None if ys[0] is None else torch.from_numpy(np.array(ys)).float(),\n",
    "        torch.tensor(weights, dtype=torch.float).unsqueeze(1),\n",
    "        None if lt_masks[0] is None else torch.from_numpy(np.array(lt_masks)),\n",
    "        None if gt_masks[0] is None else torch.from_numpy(np.array(gt_masks)),\n",
    "    )\n",
    "\n",
    "\n",
    "# See also chemprop.data.collate.collate_multicomponent\n",
    "def collate_mixture(batches: Iterable[Iterable[ComponentDatum | Datum]]) -> MixtureBatch:\n",
    "    tbs = [\n",
    "        collate_batch(batch) if isinstance(batch[0], Datum) else collate_component(batch)\n",
    "        for batch in zip(*batches)\n",
    "    ]\n",
    "\n",
    "    return MixtureBatch(\n",
    "        [tb.bmg for tb in tbs],\n",
    "        [tb.V_d for tb in tbs],\n",
    "        tbs[0].X_d,\n",
    "        tbs[0].Y,\n",
    "        tbs[0].w,\n",
    "        tbs[0].lt_mask,\n",
    "        tbs[0].gt_mask,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ComponentDatapoint(MoleculeDatapoint):\n",
    "    w_fp: np.ndarray | None = None\n",
    "    \"\"\"the weight of the molecule's learned fingerprint when averaging in the mixture\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ComponentDataset(MoleculeDataset, Dataset[ComponentMolGraph]):\n",
    "    data: list[ComponentDatapoint]\n",
    "\n",
    "    @property\n",
    "    def w_fps(self) -> np.ndarray:\n",
    "        return np.array([d.w_fp for d in self.data])\n",
    "\n",
    "    def __getitem__(self, idx: int) -> ComponentDatum:\n",
    "        d = self.data[idx]\n",
    "        mg = self.mg_cache[idx]\n",
    "        mg = ComponentMolGraph(w_fp=d.w_fp, *mg)\n",
    "\n",
    "        return ComponentDatum(\n",
    "            mg, self.V_ds[idx], self.X_d[idx], self.Y[idx], d.weight, d.lt_mask, d.gt_mask\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass(repr=False, eq=False)\n",
    "class MixtureDataset(MulticomponentDataset):\n",
    "    datasets: list[MoleculeDataset | ReactionDataset | ComponentDataset]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> list[ComponentDatum | Datum]:\n",
    "        return [dset[idx] for dset in self.datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureMessagePassing(nn.Module, HyperparametersMixin):\n",
    "    r\"\"\"A :class:`MixtureMessagePassing` encodes a batch of mixtures by passing messages along\n",
    "    molecules constructing a fully connected graph to model intermolecular interactions.\n",
    "\n",
    "    It implements the following operation:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        h_v^{(0)} &= \\tau \\left( \\mathbf{W}_i(x_v) \\right) \\\\\n",
    "        m_v^{(t)} &= \\sum_{u \\in \\mathcal{w \\in V \\setminu v} h_w^{(t-1)} \\\\\n",
    "        h_v^{(T)} &= \\tau\\left(h_v^{(0)} + \\mathbf{W}_h m_v^{(t-1)}\\right) \\\\\n",
    "\n",
    "    where :math:`\\tau` is the activation function; :math:`\\mathbf{W}_i`, :math:`\\mathbf{W}_h` are learned weight matrices; :math:`e_{vw}` is the feature vector of the\n",
    "    bond between molecules :math:`v` and :math:`w`; :math:`x_v` is the feature vector of molecule :math:`v`;\n",
    "    :math:`h_v^{(t)}` is the hidden representation of atom :math:`v` at iteration :math:`t`;\n",
    "    :math:`m_v^{(t)}` is the message received by atom :math:`v` at iteration :math:`t`; and\n",
    "    :math:`t \\in \\{1, \\dots, T\\}` is the number of message passing iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_v: int = DEFAULT_HIDDEN_DIM,\n",
    "        d_e: int | None = None,\n",
    "        d_h: int = DEFAULT_HIDDEN_DIM,\n",
    "        d_vd: int | None = None,\n",
    "        bias: bool = False,\n",
    "        depth: int = 1,\n",
    "        activation: str | Activation = Activation.RELU,\n",
    "    ):        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.hparams[\"cls\"] = self.__class__\n",
    "\n",
    "        self.depth = depth\n",
    "        self.tau = get_activation_function(activation)\n",
    "        \n",
    "        self.W_i = nn.Linear(d_v, d_h, bias)\n",
    "        self.W_h = nn.Linear(d_h, d_h, bias) # TODO consider E\n",
    "        self.W_o = None\n",
    "        self.W_d = None\n",
    "\n",
    "    def initialize(self, V: Tensor) -> Tensor:\n",
    "        return self.W_i(V)\n",
    "\n",
    "    def message(self, H: Tensor):\n",
    "        # assume fully connected graph, TODO\n",
    "        H = torch.transpose(H, 0, 1) # b x n x d\n",
    "        M_t = H.unsqueeze(2).expand(-1, -1, H.size(1), -1)\n",
    "        M_t = self.W_h(M_t)\n",
    "        mask = ~torch.eye(H.size(1), dtype=bool, device=H.device).unsqueeze(0) # exclude self-loops\n",
    "        M_t = (M_t * mask.unsqueeze(-1)).sum(dim=1)\n",
    "        M_t = torch.transpose(M_t, 0, 1)\n",
    "        return M_t\n",
    "\n",
    "    def update(self, M_t: Tensor, H_0: Tensor):\n",
    "        H_t = self.tau(H_0 + M_t)\n",
    "        return H_t\n",
    "\n",
    "    def finalize(self, H_t: Tensor):\n",
    "        return [h for h in H_t]\n",
    "\n",
    "    def forward(self, V: list[Tensor]):\n",
    "        H_0 = self.initialize(torch.stack(V))\n",
    "        H = self.tau(H_0)\n",
    "        for _ in range(self.depth):\n",
    "            M = self.message(H)\n",
    "            H = self.update(M, H_0)\n",
    "\n",
    "        return self.finalize(H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureAggregation(nn.Module, HasHParams):\n",
    "    output_dim: int\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        graph_agg: Aggregation, \n",
    "        groups: Sequence[Sequence[int]], \n",
    "        fp_dims: Sequence[int], \n",
    "        mixmp: MixtureMessagePassing | None, \n",
    "        *args, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hparams = {\n",
    "            \"cls\": self.__class__,\n",
    "            \"groups\": groups,\n",
    "            \"fp_dims\": fp_dims,\n",
    "        }\n",
    "        self.graph_agg = graph_agg\n",
    "        self.groups = groups\n",
    "        self.fp_dims = fp_dims\n",
    "        self.mixmp = mixmp\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(\n",
    "        self, Hs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Aggregate component representations into a mixture representation\"\"\"\n",
    "\n",
    "class WeightedSumAggregation(MixtureAggregation):\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims[group[0]] for group in self.groups)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        Hs = [self.graph_agg(H_v, bmg.batch) for H_v, bmg in zip(H_vs, bmgs)]\n",
    "        if not (self.mixmp is None):\n",
    "            Hs = self.mixmp(Hs)\n",
    "        combined_Hs = []\n",
    "        for group in self.groups:\n",
    "            if len(group) == 1:\n",
    "                combined_Hs.append(Hs[group[0]])\n",
    "                continue\n",
    "\n",
    "            group_Hs = torch.stack([Hs[idx] for idx in group])  # n x b x d\n",
    "            group_w_fps = torch.stack([bmgs[idx].w_fps for idx in group])  # n x b\n",
    "            # n: num. components in group, b: num. comp. in batch, d: output dim of message passing\n",
    "            combined_H = torch.einsum(\"nb,nbd->bd\", group_w_fps, group_Hs)\n",
    "            combined_Hs.append(combined_H)\n",
    "        return torch.cat(combined_Hs, 1)\n",
    "\n",
    "class ConcatAggregation(MixtureAggregation):\n",
    "    @property\n",
    "    def components_in_mixture(self) -> set[int]:\n",
    "        return {idx for group in self.groups if len(group) > 1 for idx in group}\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims) + len(self.components_in_mixture)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        Hs = [self.graph_agg(H_v, bmg.batch) for H_v, bmg in zip(H_vs, bmgs)]\n",
    "        if not (self.mixmp is None):\n",
    "            Hs = self.mixmp(Hs)\n",
    "        w_fps = torch.stack([bmgs[idx].w_fps for idx in self.components_in_mixture], dim=1)\n",
    "        return torch.cat(Hs + [w_fps], 1)\n",
    "\n",
    "class DeepsetsAggregation(MixtureAggregation):\n",
    "    r\"\"\"Deep sets aggregation of the graph-level representation:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf h = \\mathrm{MLP_{g}}(\\sum_{c \\in C} \\mathrm{MLP_{l}}(\\mathbf h_c))\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, graph_agg: Aggregation, groups: Sequence[Sequence[int]], fp_dims: Sequence[int], \n",
    "        mixmp: MixtureMessagePassing | None, *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(graph_agg, groups, fp_dims, mixmp, *args, **kwargs)\n",
    "        \n",
    "        self.MLPs_local = nn.ModuleList([])\n",
    "        self.MLPs_global = nn.ModuleList([])\n",
    "        for group in groups:\n",
    "            # TODO: allow to set hparams for MLP by kwargs (e.g., hidden_dim, n_layers)\n",
    "            hidden_dim = self.fp_dims[group[0]]\n",
    "            self.MLPs_local.append(\n",
    "                nn.Sequential(\n",
    "                nn.Linear(self.fp_dims[group[0]], hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, self.fp_dims[group[0]]),\n",
    "                )\n",
    "            )\n",
    "            self.MLPs_global.append(\n",
    "                nn.Sequential(\n",
    "                nn.Linear(self.fp_dims[group[0]], hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, self.fp_dims[group[0]]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims[group[0]] for group in self.groups)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        Hs = [self.graph_agg(H_v, bmg.batch) for H_v, bmg in zip(H_vs, bmgs)]\n",
    "        if not (self.mixmp is None):\n",
    "            Hs = self.mixmp(Hs)\n",
    "        combined_Hs = []\n",
    "        for g_idx, group in enumerate(self.groups):\n",
    "            if len(group) == 1:\n",
    "                combined_Hs.append(self.MLPs_global[g_idx](Hs[group[0]]))\n",
    "                continue\n",
    "            group_w_Hs = torch.stack([self.MLPs_local[g_idx](bmgs[idx].w_fps.unsqueeze(1) * Hs[idx]) for idx in group])  # n x b x d\n",
    "            combined_H = torch.sum(group_w_Hs, dim=0)\n",
    "            combined_Hs.append(self.MLPs_global[g_idx](combined_H))\n",
    "        return torch.cat(combined_Hs, 1)\n",
    "\n",
    "class AttentiveAggregation(MixtureAggregation):\n",
    "    r\"\"\"Attentive aggregation of the graph-level representation:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf h = \\sum_{c \\in C} \\alpha_c \\mathbf h_c\n",
    "\n",
    "        \\alpha_c = \\mathrm{softmax}(\\mathbf h_c)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, graph_agg: Aggregation, groups: Sequence[Sequence[int]], fp_dims: Sequence[int], \n",
    "        mixmp: MixtureMessagePassing | None, *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(graph_agg, groups, fp_dims, mixmp, *args, **kwargs)\n",
    "        \n",
    "        self.Ws_a = nn.ModuleList([\n",
    "            nn.Linear(self.fp_dims[group[0]], 1) for group in groups\n",
    "            ])\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims[group[0]] for group in self.groups)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        Hs = [self.graph_agg(H_v, bmg.batch) for H_v, bmg in zip(H_vs, bmgs)]\n",
    "        if not (self.mixmp is None):\n",
    "            Hs = self.mixmp(Hs)\n",
    "        combined_Hs = []\n",
    "        for g_idx, group in enumerate(self.groups):\n",
    "            if len(group) == 1:\n",
    "                combined_Hs.append((Hs[group[0]]))\n",
    "                continue\n",
    "            w_Hs = torch.stack([bmgs[idx].w_fps.unsqueeze(1) * Hs[idx] for idx in group])  # n x b x d\n",
    "            attention_logits = self.Ws_a[g_idx](w_Hs).exp()\n",
    "            Z = torch.sum(attention_logits, dim=0, keepdim=False)\n",
    "            alphas = attention_logits / Z\n",
    "            combined_H = torch.sum(alphas * w_Hs, dim=0)\n",
    "            combined_Hs.append(combined_H)\n",
    "        return torch.cat(combined_Hs, 1)\n",
    "\n",
    "class Set2SetAggregation(MixtureAggregation):\n",
    "    r\"\"\"Set2Set aggregation of the graph-level representation:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{q}_t &= \\mathrm{LSTM}(\\mathbf{q}^{*}_{t-1})\n",
    "\n",
    "        \\alpha_{c,t} &= \\mathrm{softmax}(\\mathbf{h}_c \\cdot \\mathbf{q}_t)\n",
    "\n",
    "        \\mathbf{r}_t &= \\sum_{c=1}^C \\alpha_{c,t} \\mathbf{h}_c\n",
    "\n",
    "        \\mathbf{q}^{*}_t &= \\mathbf{q}_t \\, \\Vert \\, \\mathbf{r}_t,\n",
    "\n",
    "    where :math:`\\mathbf{q}^{*}_T` defines the output of the layer with twice\n",
    "    the dimensionality as the input.\n",
    "    \n",
    "    Note: This implementation follows PyTorch Geometric (cf. https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/aggr/set2set.html#Set2Set) and is based on `\"Order Matters: Sequence to sequence for\n",
    "    Sets\" <https://arxiv.org/abs/1511.06391>`_ paper.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, graph_agg: Aggregation, groups: Sequence[Sequence[int]], fp_dims: Sequence[int], \n",
    "        mixmp: MixtureMessagePassing | None, *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(graph_agg, groups, fp_dims, mixmp, *args, **kwargs)\n",
    "        \n",
    "        # TODO: allow to set hparams for Set2Set by kwargs (e.g., processing steps)\n",
    "        self.processing_steps = 3\n",
    "        self.lstms = nn.ModuleList([])\n",
    "        for group in groups:\n",
    "            in_channels = self.fp_dims[group[0]]\n",
    "            out_channels = self.fp_dims[group[0]] * 2\n",
    "            self.lstms.append(\n",
    "                torch.nn.LSTM(out_channels, in_channels, **kwargs)\n",
    "                )\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims[group[0]] * 2 if len(group) > 1 else self.fp_dims[group[0]] for group in self.groups)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        Hs = [self.graph_agg(H_v, bmg.batch) for H_v, bmg in zip(H_vs, bmgs)]\n",
    "        if not (self.mixmp is None):\n",
    "            Hs = self.mixmp(Hs)\n",
    "        combined_Hs = []\n",
    "        for g_idx, group in enumerate(self.groups):\n",
    "            if len(group) == 1:\n",
    "                combined_Hs.append((Hs[group[0]]))\n",
    "                continue\n",
    "            \n",
    "            w_Hs = torch.stack([bmgs[idx].w_fps.unsqueeze(1) * Hs[idx] for idx in group]) \n",
    "            w_Hs = torch.transpose(w_Hs, 0, 1) # b x n x d\n",
    "            b_din = w_Hs.size(0)\n",
    "            d_dim = w_Hs.size(-1)\n",
    "\n",
    "            h = (w_Hs.new_zeros((self.lstms[g_idx].num_layers, b_din, d_dim)),\n",
    "                w_Hs.new_zeros((self.lstms[g_idx].num_layers, b_din, d_dim)))\n",
    "            q_star = w_Hs.new_zeros(b_din, d_dim * 2)\n",
    "\n",
    "            for _ in range(self.processing_steps):\n",
    "                q, h = self.lstms[g_idx](q_star.unsqueeze(0), h)\n",
    "\n",
    "                q = q.squeeze(0) # b x d\n",
    "                e = torch.sum(w_Hs * q.unsqueeze(1), dim=2) # b x n\n",
    "                a = torch.softmax(e, dim=1) # b x n\n",
    "                r = torch.sum(w_Hs * a.unsqueeze(2), dim=1) # b x d\n",
    "                q_star = torch.cat([q, r], dim=1) # b x 2*d\n",
    "\n",
    "            combined_Hs.append(q_star)\n",
    "        return torch.cat(combined_Hs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureMPNN(MulticomponentMPNN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_passing: MulticomponentMessagePassing,\n",
    "        agg: Aggregation,\n",
    "        predictor: Predictor,\n",
    "        mix_mpn: MixtureMessagePassing | None = None,\n",
    "        batch_norm: bool = False,\n",
    "        metrics: Iterable[ChempropMetric] | None = None,\n",
    "        warmup_epochs: int = 2,\n",
    "        init_lr: float = 1e-4,\n",
    "        max_lr: float = 1e-3,\n",
    "        final_lr: float = 1e-4,\n",
    "        X_d_transform: ScaleTransform | None = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            message_passing,\n",
    "            agg,\n",
    "            predictor,\n",
    "            batch_norm,\n",
    "            metrics,\n",
    "            warmup_epochs,\n",
    "            init_lr,\n",
    "            max_lr,\n",
    "            final_lr,\n",
    "            X_d_transform,\n",
    "        )\n",
    "        self.agg: MixtureAggregation\n",
    "\n",
    "    def fingerprint(\n",
    "        self,\n",
    "        bmgs: Iterable[BatchComponentMolGraph | BatchMolGraph],\n",
    "        V_ds: Iterable[Tensor],\n",
    "        X_d: Tensor | None = None,\n",
    "    ) -> Tensor:\n",
    "        H_vs: list[Tensor] = self.message_passing(bmgs, V_ds)\n",
    "        H = self.agg(H_vs, bmgs)\n",
    "        H = self.bn(H)\n",
    "        return H if X_d is None else torch.cat((H, X_d), 1)\n",
    "\n",
    "    @classmethod\n",
    "    def _load(cls, path, map_location, **submodules):\n",
    "        d = torch.load(path, map_location, weights_only=False)\n",
    "\n",
    "        try:\n",
    "            hparams = d[\"hyper_parameters\"]\n",
    "            state_dict = d[\"state_dict\"]\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Could not find hyper parameters and/or state dict in {path}.\")\n",
    "\n",
    "        hparams[\"message_passing\"][\"blocks\"] = [\n",
    "            block_hparams.pop(\"cls\")(**block_hparams)\n",
    "            for block_hparams in hparams[\"message_passing\"][\"blocks\"]\n",
    "        ]\n",
    "        graph_agg_hparams = hparams[\"agg\"][\"graph_agg\"]\n",
    "        hparams[\"agg\"][\"graph_agg\"] = graph_agg_hparams.pop(\"cls\")(**graph_agg_hparams)\n",
    "        mixmp_hparams = hparams[\"agg\"][\"mixmp\"]\n",
    "        hparams[\"agg\"][\"mixmp\"] = mixmp_hparams.pop(\"cls\")(**mixmp_hparams)\n",
    "        submodules |= {\n",
    "            key: hparams[key].pop(\"cls\")(**hparams[key])\n",
    "            for key in (\"message_passing\", \"agg\", \"predictor\")\n",
    "            if key not in submodules\n",
    "        }\n",
    "\n",
    "        if not hasattr(submodules[\"predictor\"].criterion, \"_defaults\"):\n",
    "            submodules[\"predictor\"].criterion = submodules[\"predictor\"].criterion.__class__(\n",
    "                task_weights=submodules[\"predictor\"].criterion.task_weights\n",
    "            )\n",
    "\n",
    "        return submodules, state_dict, hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemprop_dir = Path.cwd().parent\n",
    "input_path = (\n",
    "    chemprop_dir / \"tests\" / \"data\" / \"regression\" / \"mol+mol\" / \"mol+mol.csv\"\n",
    ")  # path to your data .csv file containing SMILES strings and target values\n",
    "smiles_columns = [\"smiles\", \"solvent\"]  # name of the column containing SMILES strings\n",
    "target_columns = [\"peakwavs_max\"]  # list of names of the columns containing targets\n",
    "df_input = pd.read_csv(input_path)\n",
    "smiss = df_input.loc[:, smiles_columns].values\n",
    "ys = df_input.loc[:, target_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [[MoleculeDatapoint.from_smi(smis[0], y) for smis, y in zip(smiss, ys)]]\n",
    "all_data += [[ComponentDatapoint.from_smi(smis[0], w_fp=0.1) for smis in smiss]]\n",
    "all_data += [[ComponentDatapoint.from_smi(smis[1], w_fp=0.9) for smis in smiss]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The return type of make_split_indices has changed in v2.1 - see help(make_split_indices)\n"
     ]
    }
   ],
   "source": [
    "component_to_split_by = 0  # index of the component to use for structure based splits\n",
    "mols = [d.mol for d in all_data[component_to_split_by]]\n",
    "train_indices, val_indices, test_indices = make_split_indices(mols, \"random\", (0.8, 0.1, 0.1))\n",
    "train_data, val_data, test_data = split_data_by_indices(\n",
    "    all_data, train_indices, val_indices, test_indices\n",
    ")\n",
    "train_data = train_data[0]\n",
    "val_data = val_data[0]\n",
    "test_data = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = [\n",
    "    MoleculeDataset(train_data[0]),\n",
    "    ComponentDataset(train_data[1]),\n",
    "    ComponentDataset(train_data[2]),\n",
    "]\n",
    "val_datasets = [\n",
    "    MoleculeDataset(val_data[0]),\n",
    "    ComponentDataset(val_data[1]),\n",
    "    ComponentDataset(val_data[2]),\n",
    "]\n",
    "test_datasets = [\n",
    "    MoleculeDataset(test_data[0]),\n",
    "    ComponentDataset(test_data[1]),\n",
    "    ComponentDataset(test_data[2]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mcdset = MixtureDataset(train_datasets)\n",
    "scaler = train_mcdset.normalize_targets()\n",
    "val_mcdset = MixtureDataset(val_datasets)\n",
    "val_mcdset.normalize_targets(scaler)\n",
    "test_mcdset = MixtureDataset(test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_mcdset, batch_size=10, shuffle=True, collate_fn=collate_mixture)\n",
    "val_loader = DataLoader(val_mcdset, batch_size=10, shuffle=False, collate_fn=collate_mixture)\n",
    "test_loader = DataLoader(test_mcdset, batch_size=10, shuffle=False, collate_fn=collate_mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmp = MulticomponentMessagePassing(blocks=[BondMessagePassing()], n_components=3, shared=True)\n",
    "mixmp = MixtureMessagePassing(d_v=mcmp.blocks[0].output_dim, d_h=mcmp.blocks[0].output_dim, depth=1)\n",
    "\n",
    "graph_agg = MeanAggregation()\n",
    "name_agg = \"weightedsum\"\n",
    "\n",
    "match name_agg:\n",
    "    case \"weightedsum\":\n",
    "        mixagg = WeightedSumAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case \"cat\":\n",
    "        mixagg = ConcatAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case \"deepsets\":\n",
    "        mixagg = DeepsetsAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case \"attentive\":\n",
    "        mixagg = AttentiveAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case \"set2set\":\n",
    "        mixagg = Set2SetAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case _:\n",
    "        raise ValueError(f\"MixtureAggregation {name_agg} not implemented yet.\")\n",
    "\n",
    "output_transform = UnscaleTransform.from_standard_scaler(scaler)\n",
    "ffn = RegressionFFN(input_dim=mixagg.output_dim, output_transform=output_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtureMPNN(\n",
       "  (message_passing): MulticomponentMessagePassing(\n",
       "    (blocks): ModuleList(\n",
       "      (0-2): 3 x BondMessagePassing(\n",
       "        (W_i): Linear(in_features=86, out_features=300, bias=False)\n",
       "        (W_h): Linear(in_features=300, out_features=300, bias=False)\n",
       "        (W_o): Linear(in_features=372, out_features=300, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (tau): ReLU()\n",
       "        (V_d_transform): Identity()\n",
       "        (graph_transform): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (agg): WeightedSumAggregation(\n",
       "    (graph_agg): MeanAggregation()\n",
       "  )\n",
       "  (bn): Identity()\n",
       "  (predictor): RegressionFFN(\n",
       "    (ffn): MLP(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Linear(in_features=300, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (criterion): MSE(task_weights=[[1.0]])\n",
       "    (output_transform): UnscaleTransform()\n",
       "  )\n",
       "  (X_d_transform): Identity()\n",
       "  (metrics): ModuleList(\n",
       "    (0-1): 2 x MSE(task_weights=[[1.0]])\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcmpnn = MixtureMPNN(mcmp, mixagg, ffn)\n",
    "mcmpnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rwth1232/anaconda3/envs/chemprop/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/rwth1232/anaconda3/envs/chemprop/lib/python3.1 ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=False,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rwth1232/anaconda3/envs/chemprop/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /rwthfs/rz/cluster/home/rwth1232/chemprop_pool/chemprop/examples/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/rwth1232/anaconda3/envs/chemprop/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name            | Type                         | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | message_passing | MulticomponentMessagePassing | 227 K  | train\n",
      "1 | agg             | WeightedSumAggregation       | 0      | train\n",
      "2 | bn              | Identity                     | 0      | train\n",
      "3 | predictor       | RegressionFFN                | 180 K  | train\n",
      "4 | X_d_transform   | Identity                     | 0      | train\n",
      "5 | metrics         | ModuleList                   | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "408 K     Trainable params\n",
      "0         Non-trainable params\n",
      "408 K     Total params\n",
      "1.633     Total estimated model params size (MB)\n",
      "27        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rwth1232/anaconda3/envs/chemprop/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 8/8 [00:00<00:00, 61.26it/s, train_loss_step=0.184, val_loss=0.725, train_loss_epoch=0.0888] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 8/8 [00:00<00:00, 49.88it/s, train_loss_step=0.184, val_loss=0.725, train_loss_epoch=0.0888]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.fit(mcmpnn, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/home/rwth1232/anaconda3/envs/chemprop/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 88.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     6055.58837890625      </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    6055.58837890625     \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = trainer.test(mcmpnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "/home/rwth1232/anaconda3/envs/chemprop/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 168.66it/s]\n"
     ]
    }
   ],
   "source": [
    "results = trainer.predict(mcmpnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[399.0343],\n",
       "         [708.6262],\n",
       "         [366.0291],\n",
       "         [397.4417],\n",
       "         [344.3376],\n",
       "         [373.5764],\n",
       "         [358.9327],\n",
       "         [567.5590],\n",
       "         [368.2164],\n",
       "         [509.5162]])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[384. ],\n",
       "       [553. ],\n",
       "       [394. ],\n",
       "       [428.2],\n",
       "       [386. ],\n",
       "       [369. ],\n",
       "       [520. ],\n",
       "       [515. ],\n",
       "       [313. ],\n",
       "       [480. ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_loader.dataset.datasets[0].Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

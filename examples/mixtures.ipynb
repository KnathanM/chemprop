{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from dataclasses import InitVar, dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Iterable, NamedTuple, Sequence, TypeAlias\n",
    "\n",
    "from lightning import pytorch as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit.Chem import Mol\n",
    "import rdkit.Chem as Chem\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from chemprop.data.collate import BatchMolGraph, collate_batch\n",
    "from chemprop.data.datapoints import MoleculeDatapoint, _DatapointMixin\n",
    "from chemprop.data.datasets import Datum, MoleculeDataset, MulticomponentDataset, ReactionDataset\n",
    "from chemprop.data.molgraph import MolGraph\n",
    "from chemprop.data.splitting import make_split_indices, split_data_by_indices\n",
    "from chemprop.featurizers import Featurizer, SimpleMoleculeMolGraphFeaturizer\n",
    "from chemprop.models import MulticomponentMPNN, multi\n",
    "from chemprop.nn.agg import Aggregation, MeanAggregation\n",
    "from chemprop.nn.hparams import HasHParams\n",
    "from chemprop.nn.message_passing import BondMessagePassing, MessagePassing, AtomMessagePassing\n",
    "from chemprop.nn.metrics import ChempropMetric, MSE, RMSE, MAE, R2Score\n",
    "from chemprop.nn.predictors import Predictor, RegressionFFN\n",
    "from chemprop.nn.transforms import ScaleTransform, UnscaleTransform\n",
    "from chemprop.nn.utils import Activation, get_activation_function\n",
    "from chemprop.utils import make_mol\n",
    "from lightning.pytorch.core.mixins import HyperparametersMixin\n",
    "from chemprop.conf import DEFAULT_ATOM_FDIM, DEFAULT_BOND_FDIM, DEFAULT_HIDDEN_DIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing *Components* as the molecules that are part of a mixture with a given composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extend the `MolGraph` class to include a global attribute `w_fp` which is the weight of the learned fingerprint of the molecule when averaging the fingerprints of components in the mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also chemprop.data.molgraph.MolGraph\n",
    "class ComponentMolGraph(NamedTuple):\n",
    "    V: np.ndarray\n",
    "    E: np.ndarray\n",
    "    edge_index: np.ndarray\n",
    "    rev_edge_index: np.ndarray\n",
    "    w_fp: float = 1.0\n",
    "    \"\"\"the weight of the component's fingerprint when combining components in the mixture\"\"\"\n",
    "\n",
    "# See also chemprop.data.datasets.Datum\n",
    "class ComponentDatum(NamedTuple):\n",
    "    mg: ComponentMolGraph\n",
    "    V_d: np.ndarray | None\n",
    "    x_d: np.ndarray | None\n",
    "    y: np.ndarray | None\n",
    "    weight: float\n",
    "    lt_mask: np.ndarray | None\n",
    "    gt_mask: np.ndarray | None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batched versions of `MolGraph` and `Datum` are created during collating datapoints. These are also extended, as well as the entire batch representing all components in the mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also chemprop.data.collate.BatchMolGraph\n",
    "@dataclass(repr=False, eq=False, slots=True)\n",
    "class BatchComponentMolGraph(BatchMolGraph):\n",
    "    mgs: InitVar[Sequence[ComponentMolGraph]]\n",
    "    w_fps: Tensor = field(init=False)\n",
    "    __is_empty: bool = field(init=False)\n",
    "\n",
    "    def __post_init__(self, mgs):    \n",
    "        # super(BatchComponentMolGraph, self).__post_init__(mgs) \n",
    "        self._BatchMolGraph__size = len(mgs)\n",
    "        self.__is_empty = True   \n",
    "        Vs = []\n",
    "        Es = []\n",
    "        edge_indexes = []\n",
    "        rev_edge_indexes = []\n",
    "        batch_indexes = []\n",
    "        w_fps = []\n",
    "\n",
    "        num_nodes = 0\n",
    "        num_edges = 0\n",
    "        for i, mg in enumerate(mgs):\n",
    "            if mg is None:\n",
    "                continue\n",
    "            Vs.append(mg.V)\n",
    "            Es.append(mg.E)\n",
    "            edge_indexes.append(mg.edge_index + num_nodes)\n",
    "            rev_edge_indexes.append(mg.rev_edge_index + num_edges)\n",
    "            batch_indexes.append([i] * len(mg.V))\n",
    "            w_fps.append(mg.w_fp)\n",
    "\n",
    "            num_nodes += mg.V.shape[0]\n",
    "            num_edges += mg.edge_index.shape[1]\n",
    "\n",
    "        self.V = torch.from_numpy(np.concatenate(Vs)).float() if len(Vs) > 0 else None\n",
    "        self.E = torch.from_numpy(np.concatenate(Es)).float() if len(Es) > 0 else None\n",
    "        self.edge_index = torch.from_numpy(np.hstack(edge_indexes)).long() if len(edge_indexes) > 0 else None\n",
    "        self.rev_edge_index = torch.from_numpy(np.concatenate(rev_edge_indexes)).long() if len(rev_edge_indexes) > 0 else None\n",
    "        self.batch = torch.tensor(np.concatenate(batch_indexes)).long() if len(batch_indexes) > 0 else None\n",
    "        self.w_fps = torch.from_numpy(np.array(w_fps)).float() if len(w_fps) > 0 else None\n",
    "\n",
    "        if len(Vs) > 0:\n",
    "            self.__is_empty = False\n",
    "\n",
    "    def to(self, device: str | torch.device):\n",
    "        if not self.is_empty():\n",
    "            super(BatchComponentMolGraph, self).to(device)\n",
    "            self.w_fps = self.w_fps.to(device)\n",
    "\n",
    "    def is_empty(self) -> bool:\n",
    "        \"\"\"whether any :class:`MolGraph`\\s are stored in this batch\"\"\"\n",
    "        return self.__is_empty\n",
    "\n",
    "# See also chemprop.data.collate.TrainingBatch\n",
    "class BatchComponentDatum(NamedTuple):\n",
    "    bmg: BatchComponentMolGraph\n",
    "    V_d: Tensor | None\n",
    "    X_d: Tensor | None\n",
    "    Y: Tensor | None\n",
    "    w: Tensor\n",
    "    lt_mask: Tensor | None\n",
    "    gt_mask: Tensor | None\n",
    "\n",
    "# See also chemprop.data.collate.collate_batch\n",
    "def collate_component(batch: Iterable[Datum]) -> BatchComponentDatum:\n",
    "    mgs, V_ds, x_ds, ys, weights, lt_masks, gt_masks = zip(*batch)\n",
    "\n",
    "    return BatchComponentDatum(\n",
    "        BatchComponentMolGraph(mgs),\n",
    "        None if V_ds[0] is None else torch.from_numpy(np.concatenate(V_ds)).float(),\n",
    "        None if x_ds[0] is None else torch.from_numpy(np.array(x_ds)).float(),\n",
    "        None if ys[0] is None else torch.from_numpy(np.array(ys)).float(),\n",
    "        torch.tensor(weights, dtype=torch.float).unsqueeze(1),\n",
    "        None if lt_masks[0] is None else torch.from_numpy(np.array(lt_masks)),\n",
    "        None if gt_masks[0] is None else torch.from_numpy(np.array(gt_masks)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also account for the ``w_fp`` for the datapoint and dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ComponentDatapoint(MoleculeDatapoint):\n",
    "    w_fp: np.ndarray | None = None\n",
    "    \"\"\"the weight of the molecule's learned fingerprint when averaging in the mixture\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ComponentDataset(MoleculeDataset, Dataset[ComponentMolGraph]):\n",
    "    data: list[ComponentDatapoint]\n",
    "\n",
    "    @property\n",
    "    def w_fps(self) -> np.ndarray:\n",
    "        return np.array([d.w_fp for d in self.data])\n",
    "\n",
    "    def __getitem__(self, idx: int) -> ComponentDatum:\n",
    "        d = self.data[idx]\n",
    "        if d.mol:\n",
    "            mg = self.mg_cache[idx] \n",
    "            mg = ComponentMolGraph(w_fp=d.w_fp, *mg)\n",
    "        else:\n",
    "            mg = None\n",
    "\n",
    "        return ComponentDatum(\n",
    "            mg, self.V_ds[idx], self.X_d[idx], self.Y[idx], d.weight, d.lt_mask, d.gt_mask\n",
    "        )\n",
    "\n",
    "@dataclass(repr=False, eq=False)\n",
    "class MixtureDataset(MulticomponentDataset): # TODO, potentially rename to anvoid confusion with MixtureGraphDataset?\n",
    "    datasets: list[MoleculeDataset | ReactionDataset | ComponentDataset]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> list[ComponentDatum | Datum]:\n",
    "        return [dset[idx] for dset in self.datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing *Mixtures* as objects that represent molecules and their interactions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define the ``MixtureGraph`` class, i.e., nodes representing molecules and edges representing intermolecular interactions, together with the corresponding ``MixtureDatum``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also chemprop.data.molgraph.MolGraph\n",
    "class MixtureGraph(NamedTuple):\n",
    "    V: np.ndarray\n",
    "    E: np.ndarray\n",
    "    edge_index: np.ndarray\n",
    "    rev_edge_index: np.ndarray\n",
    "\n",
    "# See also chemprop.data.datasets.Datum\n",
    "class MixtureDatum(NamedTuple):\n",
    "    # TODO: we could already make list of Molecule/ComponentMolGraphs here and store molecular information of the mixture at this level instead in stacked component-specific datasets\n",
    "    mg: MixtureGraph\n",
    "    V_d: np.ndarray | None\n",
    "    x_d: np.ndarray | None\n",
    "    y: np.ndarray | None\n",
    "    weight: float\n",
    "    lt_mask: np.ndarray | None\n",
    "    gt_mask: np.ndarray | None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the ``SimpleMixtureGraphFeaturizer`` for ``MixtureGraph`` objects that takes in the molecules of a mixture, constructs a mixture graph, and adds descriptors for intermolecular interactions to the edge features (currently only considering Hydrogen bonding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from chemprop.featurizers.base import VectorFeaturizer\n",
    "from chemprop.utils.utils import EnumMapping\n",
    "from enum import auto\n",
    "from chemprop.featurizers.molgraph.mixins import _MolGraphFeaturizerMixin\n",
    "from chemprop.featurizers.base import GraphFeaturizer\n",
    "\n",
    "class MultiHotMolinteractionFeaturizer(VectorFeaturizer[Sequence[Mol]]):\n",
    "    \"\"\"A :class:`MultiHotMolinteractionFeaturizer` uses a multi-hot encoding to featurize interactions in mixture graphs.\n",
    "\n",
    "    The generated interaction features are ordered as follows:\n",
    "    * hydrogen bonding\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_hbond_num : int\n",
    "        the number for maximum hydrogen bonds.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        h_bonds: Sequence[int],\n",
    "    ):\n",
    "        self.h_bonds = {i: i for i in h_bonds}\n",
    "\n",
    "        self._subfeats: list[dict] = [\n",
    "            self.h_bonds,\n",
    "        ]\n",
    "        subfeat_sizes = [\n",
    "            1 + len(self.h_bonds),\n",
    "        ]\n",
    "        self.__size = sum(subfeat_sizes)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.__size\n",
    "\n",
    "    def __call__(self, mols: Sequence[Mol] | None) -> np.ndarray:\n",
    "        x = np.zeros(self.__size)\n",
    "\n",
    "        if mols is None:\n",
    "            return x\n",
    "        if len(mols) == 1:\n",
    "            feats = [\n",
    "                self._descriptor_intra_HB(mols[0])\n",
    "            ]\n",
    "        elif len(mols) > 1:\n",
    "            feats = [\n",
    "                self._descriptor_inter_HB(mols)\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\"Tried to featurize molecular interactions but empty list of mols was provided.\")\n",
    "\n",
    "        i = 0\n",
    "        for feat, choices in zip(feats, self._subfeats):\n",
    "            j = choices.get(feat, len(choices))\n",
    "            x[i + j] = 1\n",
    "            i += len(choices) + 1\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _descriptor_intra_HB(self, mol):\n",
    "        # From https://github.com/edgarsmdn/GH-GNN/blob/main/scr/models/utilities/mol2graph.py\n",
    "        # Intra hydrogen-bond acidity and basicity\n",
    "        return min(Chem.rdMolDescriptors.CalcNumHBA(mol), Chem.rdMolDescriptors.CalcNumHBD(mol))\n",
    "\n",
    "    def _descriptor_inter_HB(self, mol_list):\n",
    "        # From https://github.com/edgarsmdn/GH-GNN/blob/main/scr/models/utilities/mol2graph.py\n",
    "        # Inter hydrogen-bond acidity and basicity\n",
    "        if len(mol_list) != 2:\n",
    "            raise ValueError(f\"Interaction can only be calculated between two molecules. {len(mol_list)} molecules are given.\")\n",
    "        mol_1, mol_2 = mol_list[0], mol_list[1]\n",
    "        return min(Chem.rdMolDescriptors.CalcNumHBA(mol_1), Chem.rdMolDescriptors.CalcNumHBD(mol_2)) +  min(Chem.rdMolDescriptors.CalcNumHBA(mol_2), Chem.rdMolDescriptors.CalcNumHBD(mol_1))\n",
    "\n",
    "    @classmethod\n",
    "    def hb(cls, max_hbond_num: int = 3):\n",
    "        \"\"\"The implementation of molecular interactions based on hydrogen bonding (hb) used in [1].\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_hbond_num : int, default=3\n",
    "            Include a bit for all hydrogen bond numbers in the interval :math:`[1, \\mathtt{max\\_hbond\\_num}]`\n",
    "\n",
    "        References\n",
    "        -----------\n",
    "        .. [1] Medina, E. I. S., Linke, S., Stoll, M., & Sundmacher, K. (2023). Gibbs–Helmholtz graph neural network: capturing the temperature dependency of activity coefficients at infinite dilution. Digital Discovery, 2(3), 781-798. https://doi.org/10.1039/D2DD00142J\n",
    "        \"\"\"\n",
    "\n",
    "        return cls(\n",
    "            h_bonds=list(range(0, max_hbond_num)),\n",
    "        )\n",
    "\n",
    "class MolinteractionFeatureMode(EnumMapping):\n",
    "    \"\"\"The mode of an atom is used for featurization into a `MolGraph`\"\"\"\n",
    "\n",
    "    HB = auto()\n",
    "\n",
    "@dataclass\n",
    "class _MixtureGraphFeaturizerMixin:\n",
    "    # mol_featurizer: VectorFeaturizer[Mol] = field(default_factory=) # TODO: we could also featurize mols and thereby provide the option to omit the MPNN on individual molecules\n",
    "    interaction_featurizer: VectorFeaturizer[list[Mol]] = field(default_factory=MultiHotMolinteractionFeaturizer.hb)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.mol_fdim = 0 #len(self.mol_featurizer), TODO\n",
    "        self.interaction_fdim = len(self.interaction_featurizer)\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> tuple[int, int]:\n",
    "        \"\"\"the feature dimension of the molecules and interactions, respectively, of `MixtureGraph`s generated by\n",
    "        this featurizer\"\"\"\n",
    "        return self.mol_fdim, self.interaction_fdim\n",
    "\n",
    "@dataclass\n",
    "class SimpleMixtureGraphFeaturizer(_MixtureGraphFeaturizerMixin, GraphFeaturizer[Sequence[Mol]]):\n",
    "    \"\"\"A :class:`SimpleMoleculeMolGraphFeaturizer` is the default implementation of a\n",
    "    :class:`MoleculeMolGraphFeaturizer`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    interaction_featurizer : InteractionFeaturizer, default=MultiHotMolinteractionFeaturizer()\n",
    "        the featurizer with which to calculate feature representations of the molecular interactions in a given\n",
    "        mixture\n",
    "    extra_interaction_fdim : int, default=0\n",
    "        the dimension of the additional features that will be concatenated onto the calculated\n",
    "        features of each interaction\n",
    "    \"\"\"\n",
    "\n",
    "    extra_mol_fdim: InitVar[int] = 0\n",
    "    extra_interaction_fdim: InitVar[int] = 0\n",
    "\n",
    "    def __post_init__(self, extra_mol_fdim: int = 0, extra_interaction_fdim: int = 0):\n",
    "        super().__post_init__()\n",
    "\n",
    "        self.extra_mol_fdim = extra_mol_fdim\n",
    "        self.extra_interaction_fdim = extra_interaction_fdim\n",
    "        self.mol_fdim += self.extra_mol_fdim\n",
    "        self.interaction_fdim += self.extra_interaction_fdim\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        mols: list[Chem.Mol],\n",
    "        mol_features_extra: np.ndarray | None = None,\n",
    "        interaction_features_extra: np.ndarray | None = None,\n",
    "    ) -> MolGraph:\n",
    "        n_mols = len(mols)\n",
    "        n_interactions = int((n_mols - 1) * n_mols / 2 + n_mols)\n",
    "\n",
    "        if mol_features_extra is not None and len(mol_features_extra) != n_mols:\n",
    "            raise ValueError(\n",
    "                \"Input mixture must have same number of molecules as `len(mol_features_extra)`!\"\n",
    "                f\"got: {n_mols} and {len(mol_features_extra)}, respectively\"\n",
    "            )\n",
    "\n",
    "        if interaction_features_extra is not None and len(interaction_features_extra) != n_interactions:\n",
    "            raise ValueError(\n",
    "                \"Input mixture must have same number of interactions (n_mols * (n_mols / 2 + 1)) as `len(interaction_features_extra)`!\"\n",
    "                f\"got: {n_interactions} and {len(interaction_features_extra)}, respectively\"\n",
    "            )\n",
    "\n",
    "        if n_mols == 0:\n",
    "            V = np.zeros((1, self.mol_fdim), dtype=np.single)\n",
    "        elif mol_features_extra is not None:\n",
    "            V = mol_features_extra\n",
    "        else:\n",
    "            V = np.zeros((n_mols, self.mol_fdim), dtype=np.single)\n",
    "            \n",
    "        E = np.empty((2 * n_interactions, self.interaction_fdim))\n",
    "        edge_index = [[], []]\n",
    "        i = 0\n",
    "        for mol1_idx, mol1 in enumerate(mols):\n",
    "            for mol2_idx, mol2 in enumerate(mols):\n",
    "                # avoid duplicate interactions\n",
    "                if mol2_idx < mol1_idx: \n",
    "                    continue\n",
    "                # self-interaction\n",
    "                if mol1_idx == mol2_idx: \n",
    "                    x_e = self.interaction_featurizer([mol1])\n",
    "                # intermolecular interaction\n",
    "                else:\n",
    "                    x_e = self.interaction_featurizer([mol1, mol2])\n",
    "                if interaction_features_extra is not None:\n",
    "                    x_e = np.concatenate((x_e, interation_features_extra[mol_idx1+mol_idx2]), dtype=np.single) # the indexing is not obvious\n",
    "\n",
    "                E[i : i + 2] = x_e\n",
    "\n",
    "                edge_index[0].extend([mol1_idx, mol2_idx])\n",
    "                edge_index[1].extend([mol2_idx, mol1_idx])\n",
    "\n",
    "                i += 2\n",
    "\n",
    "        rev_edge_index = np.arange(len(E)).reshape(-1, 2)[:, ::-1].ravel()\n",
    "        edge_index = np.array(edge_index, int)\n",
    "\n",
    "        return MixtureGraph(V, E, edge_index, rev_edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching of mixture graphs is analogous to molecular graph batching. We introduce similar classes to avoid confusion of mixture graph with molecular graph objects and to make future extensions straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchMixtureGraph(BatchMolGraph):\n",
    "    \"\"\"A :class:`BatchMixtureGraph` represents a batch of individual :class:`MixtureGraph`\\s.\n",
    "\n",
    "    It has all the attributes of a ``BatchMolGraph``.\n",
    "    \"\"\"\n",
    "    mgs: InitVar[Sequence[MixtureGraph]]\n",
    "\n",
    "\n",
    "# See also chemprop.data.collate.MulticomponentTrainingBatch\n",
    "class MixtureBatch(NamedTuple):\n",
    "    bmgs: list[MixtureGraph]\n",
    "    V_ds: list[Tensor | None]\n",
    "    X_d: Tensor | None\n",
    "    Y: Tensor | None\n",
    "    w: Tensor\n",
    "    lt_mask: Tensor | None\n",
    "    gt_mask: Tensor | None\n",
    "\n",
    "\n",
    "# See also chemprop.data.collate.TrainingBatch\n",
    "class BatchMixtureDatum(NamedTuple):\n",
    "    bmg: BatchMixtureGraph\n",
    "    V_d: Tensor | None\n",
    "    X_d: Tensor | None\n",
    "    Y: Tensor | None\n",
    "    w: Tensor\n",
    "    lt_mask: Tensor | None\n",
    "    gt_mask: Tensor | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_mixturegraph(batch: Iterable[Datum]) -> BatchMixtureDatum:\n",
    "    mgs, V_ds, x_ds, ys, weights, lt_masks, gt_masks = zip(*batch)\n",
    "\n",
    "    return BatchMixtureDatum(\n",
    "        BatchMixtureGraph(mgs),\n",
    "        None if V_ds[0] is None else torch.from_numpy(np.concatenate(V_ds)).float(),\n",
    "        None if x_ds[0] is None else torch.from_numpy(np.array(x_ds)).float(),\n",
    "        None if ys[0] is None else torch.from_numpy(np.array(ys)).float(),\n",
    "        torch.tensor(weights, dtype=torch.float).unsqueeze(1),\n",
    "        None if lt_masks[0] is None else torch.from_numpy(np.array(lt_masks)),\n",
    "        None if gt_masks[0] is None else torch.from_numpy(np.array(gt_masks)),\n",
    "    )\n",
    "\n",
    "# See also chemprop.data.collate.collate_multicomponent\n",
    "def collate_mixture(batches: Iterable[Iterable[ComponentDatum | Datum]]) -> MixtureBatch:\n",
    "    tbs = []\n",
    "    for batch in zip(*batches):\n",
    "        if isinstance(batch[0], MixtureDatum):\n",
    "            tbs.append(collate_mixturegraph(batch))\n",
    "        elif isinstance(batch[0], Datum):\n",
    "            tbs.append(collate_batch(batch))\n",
    "        else:\n",
    "            tbs.append(collate_component(batch))\n",
    "\n",
    "    return MixtureBatch(\n",
    "        [tb.bmg for tb in tbs],\n",
    "        #[tmixb.bmg for tmixb in tmixbs],\n",
    "        [tb.V_d for tb in tbs],\n",
    "        tbs[0].X_d,\n",
    "        tbs[0].Y,\n",
    "        tbs[0].w,\n",
    "        tbs[0].lt_mask,\n",
    "        tbs[0].gt_mask,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating mixture datapoints and datasets is based on the provided molecules in the form of SMILES strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class _MixtureDatapointMixin:\n",
    "    mols: list[Chem.Mol]\n",
    "    \"\"\"the mixture associated with this datapoint\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_smis(\n",
    "        cls, smis: str, *args, keep_h: bool = False, add_h: bool = False, **kwargs\n",
    "    ): #-> _MixtureDatapointMixin: TODO?\n",
    "        mols = [make_mol(smi, keep_h, add_h) for smi in smis if smi]\n",
    "\n",
    "        kwargs[\"name\"] = smis if \"name\" not in kwargs else kwargs[\"name\"]\n",
    "\n",
    "        return cls(mols, *args, **kwargs)\n",
    "\n",
    "@dataclass\n",
    "class MixtureDatapoint(_DatapointMixin, _MixtureDatapointMixin):\n",
    "\n",
    "    V_f: np.ndarray | None = None\n",
    "    \"\"\"a numpy array of shape ``V x d_vf``, where ``V`` is the number of molecules in the mixture, and\n",
    "    ``d_vf`` is the number of additional features that will be concatenated to molecule-level features\n",
    "    *before* message passing\"\"\"\n",
    "    E_f: np.ndarray | None = None\n",
    "    \"\"\"A numpy array of shape ``E x d_ef``, where ``E`` is the number of interactions in the mixture, and\n",
    "    ``d_ef`` is the number of additional features containing additional features that will be\n",
    "    concatenated to interaction-level features *before* message passing\"\"\"\n",
    "    V_d: np.ndarray | None = None\n",
    "    \"\"\"A numpy array of shape ``V x d_vd``, where ``V`` is the number of molecules in the mixture, and\n",
    "    ``d_vd`` is the number of additional descriptors that will be concatenated to molecule-level\n",
    "    descriptors *after* message passing\"\"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        NAN_TOKEN = 0\n",
    "        if self.V_f is not None:\n",
    "            self.V_f[np.isnan(self.V_f)] = NAN_TOKEN\n",
    "        if self.E_f is not None:\n",
    "            self.E_f[np.isnan(self.E_f)] = NAN_TOKEN\n",
    "        if self.V_d is not None:\n",
    "            self.V_d[np.isnan(self.V_d)] = NAN_TOKEN\n",
    "\n",
    "        super().__post_init__()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return 1\n",
    "\n",
    "@dataclass\n",
    "class MixtureGraphDataset(MoleculeDataset, Dataset[MixtureGraph]):\n",
    "    data: list[MixtureDatapoint]\n",
    "    featurizer: Featurizer[list[Mol], MixtureGraph] = field(default_factory=SimpleMixtureGraphFeaturizer)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> MixtureDatum:\n",
    "        d = self.data[idx]\n",
    "        mg = self.mg_cache[idx]\n",
    "        return MixtureDatum(\n",
    "            mg, self.V_ds[idx], self.X_d[idx], self.Y[idx], d.weight, d.lt_mask, d.gt_mask\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def smiles(self) -> list[list[str]]:\n",
    "        \"\"\"the SMILES strings associated with the dataset\"\"\"\n",
    "        return [[Chem.MolToSmiles(mol) for mol in d.mols] for d in self.data]\n",
    "\n",
    "    @property\n",
    "    def mols(self) -> list[list[Chem.Mol]]:\n",
    "        \"\"\"the molecules associated with the dataset\"\"\"\n",
    "        return [[mol for mol in d.mols] for d in self.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message passing with multiple components from different ``groups``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The groups provide the indices for the respective molecular/component-wise datasets. For example, ``groups = [[0], [1, 2, 3]]``, where the first group corresponds to solute (1 molecule) and the second group corresponds to solvents (3 molecules). If all molecules/components are part of a mixture, one single group should be used, i.e., ``groups = [[0, 1, 2, 3]]``.\n",
    "\n",
    "Note that the molecules within each group use the message passing block. Also, if ``shared = True``, all groups share a message passing block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticomponentMessagePassing(nn.Module, HasHParams):\n",
    "    \"\"\"A `MulticomponentMessagePassing` performs message-passing on each individual input in a\n",
    "    multicomponent input then concatenates the representation of each input to construct a\n",
    "    global representation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    blocks : Sequence[MessagePassing]\n",
    "        the invidual message-passing blocks for each input\n",
    "    groups: Sequence[Sequence[int]]\n",
    "        the indices of the molecules/components split into groups\n",
    "    shared : bool, default=False\n",
    "        whether one block will be shared among all components in an input. If not, a separate\n",
    "        block will be learned for each component.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        blocks: Sequence[MessagePassing], \n",
    "        groups: Sequence[Sequence[int]], \n",
    "        shared: bool = False,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.hparams = {\n",
    "            \"cls\": self.__class__,\n",
    "            \"blocks\": [block.hparams for block in blocks],\n",
    "            \"groups\": groups,\n",
    "            \"shared\": shared,\n",
    "        }\n",
    "\n",
    "        if len(blocks) == 0:\n",
    "            raise ValueError(\"arg 'blocks' was empty!\")\n",
    "        if groups is None:\n",
    "            raise ValueError(\"arg 'groups' was empty!\")\n",
    "\n",
    "        if shared:\n",
    "            if len(blocks) > 1:\n",
    "                logger.warning(\n",
    "                    \"More than 1 block was supplied but 'shared' was True! Using only the 0th block...\"\n",
    "                )\n",
    "            if len(groups) != sum(len(g) if isinstance(g, list) else 1 for g in groups):\n",
    "                logger.warning(\n",
    "                    \"Different groups were supplied but 'shared' was True! Using only the 0th block for all groups...\"\n",
    "                )\n",
    "        else:\n",
    "            if len(blocks) != len(groups):\n",
    "                raise ValueError(\n",
    "                    \"arg 'len(groups)' must be equal to `len(blocks)` if 'shared' is False!\"\n",
    "                    f\"got: {len(groups)} and {len(blocks)}, respectively.\"\n",
    "                )\n",
    "\n",
    "        self.groups = groups\n",
    "        self.shared = shared\n",
    "        self.blocks = nn.ModuleList()\n",
    "        # Use one message passing block for all groups\n",
    "        if shared:\n",
    "            self.blocks.extend([blocks[0]] * len(groups))\n",
    "        # Use group-wise message passing block\n",
    "        else:\n",
    "            b_idx = 0\n",
    "            for g_idx, g in enumerate(groups):\n",
    "                self.blocks.extend([blocks[g_idx]] * len(g))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.blocks)\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        d_o = sum(block.output_dim for block in self.blocks)\n",
    "\n",
    "        return d_o\n",
    "\n",
    "    def forward(self, bmgs: Iterable[BatchMolGraph], V_ds: Iterable[Tensor | None]) -> list[Tensor]:\n",
    "        \"\"\"Encode the multicomponent inputs\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bmgs : Iterable[BatchMolGraph]\n",
    "        V_ds : Iterable[Tensor | None]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[Tensor]\n",
    "            a list of tensors of shape `V x d_i` containing the respective encodings of the `i`\\th\n",
    "            component, where `d_i` is the output dimension of the `i`\\th encoder\n",
    "        \"\"\"\n",
    "        # Note: check if bmg is None in case we consider mixtures with different number of components\n",
    "        if V_ds is None:\n",
    "            return [block(bmg) if not (bmg.V is None) else None for block, bmg in zip(self.blocks, bmgs)]\n",
    "        else:\n",
    "            return [block(bmg, V_d) if not (bmg.V is None) else None for block, bmg, V_d in zip(self.blocks, bmgs, V_ds)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixture-level message passing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide two types of message passing on the mixture level:\n",
    "* On-the-fly: a fully-connected mixture graph is constructed during the forward pass of the model, whereas no additional molecular (interaction) descriptors are calculated. That is, we just enable passing information between the learned moleular fingerprints (without any edge features).\n",
    "* MixtureGraph: a ``MixtureGraphDataset`` (see above) must be generated and passed to the model. We then implement ``MolecularMessagePassing``, mimicking ``AtomMessagePassing``, i.e., node-centered message passing. We also implement ``InteractionMessagePassing``, mimicking ``BondMessagePassing``, i.e., edge-centered message passing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureMessagePassing(nn.Module, HyperparametersMixin):\n",
    "    r\"\"\"A :class:`MixtureMessagePassing` encodes a batch of mixtures by passing messages along\n",
    "    molecules constructing a fully connected graph to model intermolecular interactions.\n",
    "\n",
    "    It implements the following operation:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        h_v^{(0)} &= \\tau \\left( \\mathbf{W}_i(x_v) \\right) \\\\\n",
    "        m_v^{(t)} &= \\sum_{u \\in \\mathcal{w \\in V \\setminu v} h_w^{(t-1)} \\\\\n",
    "        h_v^{(T)} &= \\tau\\left(h_v^{(0)} + \\mathbf{W}_h m_v^{(t-1)}\\right) \\\\\n",
    "\n",
    "    where :math:`\\tau` is the activation function; :math:`\\mathbf{W}_i`, :math:`\\mathbf{W}_h` are learned weight matrices; :math:`e_{vw}` is the feature vector of the\n",
    "    bond between molecules :math:`v` and :math:`w`; :math:`x_v` is the feature vector of molecule :math:`v`;\n",
    "    :math:`h_v^{(t)}` is the hidden representation of atom :math:`v` at iteration :math:`t`;\n",
    "    :math:`m_v^{(t)}` is the message received by atom :math:`v` at iteration :math:`t`; and\n",
    "    :math:`t \\in \\{1, \\dots, T\\}` is the number of message passing iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_v: int = DEFAULT_HIDDEN_DIM,\n",
    "        d_e: int | None = None,\n",
    "        d_h: int = DEFAULT_HIDDEN_DIM,\n",
    "        d_vd: int | None = None,\n",
    "        bias: bool = False,\n",
    "        depth: int = 1,\n",
    "        activation: str | Activation = Activation.RELU,\n",
    "    ):        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.hparams[\"cls\"] = self.__class__\n",
    "\n",
    "        self.depth = depth\n",
    "        self.tau = get_activation_function(activation)\n",
    "        \n",
    "        self.W_i = nn.Linear(d_v, d_h, bias)\n",
    "        self.W_h = nn.Linear(d_h, d_h, bias) # TODO consider E\n",
    "        self.W_o = None\n",
    "        self.W_d = None\n",
    "\n",
    "    def initialize(self, V: Tensor) -> Tensor:\n",
    "        return self.W_i(V)\n",
    "\n",
    "    def message(self, H: Tensor):\n",
    "        # assume fully connected graph, TODO\n",
    "        H = torch.transpose(H, 0, 1) # b x n x d\n",
    "        M_t = H.unsqueeze(2).expand(-1, -1, H.size(1), -1) # b x n x n x d\n",
    "        M_t = self.W_h(M_t)\n",
    "        mask = ~torch.eye(H.size(1), dtype=bool, device=H.device).unsqueeze(0) # exclude self-loops (n x n)\n",
    "        M_t = (M_t * mask.unsqueeze(-1)).sum(dim=1) # b x n x d\n",
    "        M_t = torch.transpose(M_t, 0, 1) # n x b x d\n",
    "        return M_t\n",
    "\n",
    "    def update(self, M_t: Tensor, H_0: Tensor):\n",
    "        H_t = self.tau(H_0 + M_t)\n",
    "        return H_t\n",
    "\n",
    "    def finalize(self, H_t: Tensor):\n",
    "        return [h for h in H_t]\n",
    "\n",
    "    def forward(self, V: list[Tensor]):\n",
    "        H_0 = self.initialize(torch.stack(V))\n",
    "        H = self.tau(H_0)\n",
    "        for _ in range(self.depth):\n",
    "            M = self.message(H)\n",
    "            H = self.update(M, H_0)\n",
    "\n",
    "        return self.finalize(H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularMessagePassing(AtomMessagePassing):\n",
    "\n",
    "    def forward(self, bmg: BatchMixtureGraph, Hs: list[Tensor], Hs_batch: list[Tensor], V_d: Tensor | None = None) -> Tensor:\n",
    "        r\"\"\"Encode a batch of molecular graphs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bmg: BatchMixtureGraph\n",
    "            a batch of :class:`BatchMixtureGraph`s to encode\n",
    "        Hs: list[Tensor]\n",
    "            the molecular fiingerprint tensors \n",
    "        V_d : Tensor | None, default=None\n",
    "            an optional tensor of shape ``V x d_vd`` containing additional descriptors for each molecule\n",
    "            in the batch. These will be concatenated to the learned molecular descriptors and\n",
    "            transformed before the readout phase.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            a tensor of shape ``V x d_h`` or ``V x (d_h + d_vd)`` containing the encoding of each\n",
    "            molecule in the batch, depending on whether additional molecular descriptors were provided\n",
    "        \"\"\"\n",
    "        batch_size = max(H_b.max().item() for H_b in Hs_batch if not (H_b is None))\n",
    "        flat_Hs = []\n",
    "        count_n = [0 for _ in range(len(Hs_batch))]\n",
    "        flat_idx = []\n",
    "        for b in range(batch_size+1):\n",
    "            for idx, (H, H_b) in enumerate(zip(Hs, Hs_batch)):\n",
    "                if not (H is None):\n",
    "                    if (b in H_b):\n",
    "                    # TODO: add extra mol features\n",
    "                    # if the mixture graph nodes vectors store information, they correspond to extra molecular features that should be considered in message passing\n",
    "                    #if bmg.V[0].numel() != 0:\n",
    "                    #    flat_Hs.append(torch.cat((H[count_b], bmg.V[count_mg]), dim=1))\n",
    "                    # otherwise empty node vectors are overwritten\n",
    "                    #else:\n",
    "                        flat_Hs.append(H[count_n[idx]])\n",
    "                        flat_idx.append(idx)\n",
    "                        count_n[idx] += 1\n",
    "        flat_Hs = torch.stack(flat_Hs)\n",
    "        if not isinstance(bmg, BatchMixtureGraph):\n",
    "            raise TypeError(f\"MixtureGraphMessagePassing requires class :class:`BatchMixtureGraph` as input but received object of :class:`{type(bmg)}`\")\n",
    "        bmg.V = flat_Hs\n",
    "        Hs = bmg.V\n",
    "        Hs = super().forward(bmg, V_d)\n",
    "        flat_idx = torch.tensor(flat_idx)\n",
    "        Hs = [Hs[flat_idx == i] for i, _ in enumerate(Hs_batch)]\n",
    "        return Hs\n",
    "\n",
    "\n",
    "class InteractionMessagePassing(BondMessagePassing):\n",
    "\n",
    "    def forward(self, bmg: BatchMixtureGraph, Hs: list[Tensor], Hs_batch: list[Tensor], V_d: Tensor | None = None) -> Tensor:\n",
    "        r\"\"\"Encode a batch of molecular graphs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bmg: BatchMixtureGraph\n",
    "            a batch of :class:`BatchMixtureGraph`s to encode\n",
    "        Hs: list[Tensor]\n",
    "            the molecular fiingerprint tensors \n",
    "        V_d : Tensor | None, default=None\n",
    "            an optional tensor of shape ``V x d_vd`` containing additional descriptors for each molecule\n",
    "            in the batch. These will be concatenated to the learned molecular descriptors and\n",
    "            transformed before the readout phase.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            a tensor of shape ``V x d_h`` or ``V x (d_h + d_vd)`` containing the encoding of each\n",
    "            molecule in the batch, depending on whether additional molecular descriptors were provided\n",
    "        \"\"\"\n",
    "        batch_size = max(H_b.max().item() for H_b in Hs_batch if not (H_b is None))\n",
    "        flat_Hs = []\n",
    "        count_n = [0 for _ in range(len(Hs_batch))]\n",
    "        flat_idx = []\n",
    "        for b in range(batch_size+1):\n",
    "            for idx, (H, H_b) in enumerate(zip(Hs, Hs_batch)):\n",
    "                if not (H is None):\n",
    "                    if (b in H_b):\n",
    "                    # TODO: add extra mol features\n",
    "                    # if the mixture graph nodes vectors store information, they correspond to extra molecular features that should be considered in message passing\n",
    "                    #if bmg.V[0].numel() != 0:\n",
    "                    #    flat_Hs.append(torch.cat((H[count_b], bmg.V[count_mg]), dim=1))\n",
    "                    # otherwise empty node vectors are overwritten\n",
    "                    #else:\n",
    "                        flat_Hs.append(H[count_n[idx]])\n",
    "                        flat_idx.append(idx)\n",
    "                        count_n[idx] += 1\n",
    "        flat_Hs = torch.stack(flat_Hs)\n",
    "        if not isinstance(bmg, BatchMixtureGraph):\n",
    "            raise TypeError(f\"MixtureGraphMessagePassing requires class :class:`BatchMixtureGraph` as input but received object of :class:`{type(bmg)}`\")\n",
    "        bmg.V = flat_Hs\n",
    "        Hs = bmg.V\n",
    "        Hs = super().forward(bmg, V_d)\n",
    "        flat_idx = torch.tensor(flat_idx)\n",
    "        Hs = [Hs[flat_idx == i] for i, _ in enumerate(Hs_batch)]\n",
    "        return Hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``MixtureAggregation``: From molecular to mixture representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extend the ``Aggregation`` class suited for molecules to handle both molecules and their mixtures. The abstract ``MixtureAggregation`` class covers:\n",
    "* ``Aggregation``: Atom-to-molecule aggregation\n",
    "* ``MixtureMessagePassing``: mixture-level message passing (based on mixture graphs), *Note*: MixtureGraphDataset is assumed to have index `-1`,\n",
    "while the inheriting classes implement:\n",
    "* Molecule-to-mixture aggregation.\n",
    "\n",
    "We currently include: \n",
    "* ``ConcatAggregation``: Simply concatenating all molecular fingerprints and the individual compositions.\n",
    "* ``WeightedSumAggregation``: groups-wise sum of molecular fingerprints multiplied by their individual compositions\n",
    "* ``DeepsetsAggregation``: can be seen as an extension of ``WeightedSumAggregation``, whereas the individual moelcular fingerprints multiplied by the compositions pass a _local_ MLP before being summed and then the group-wise sums pass a _global_ MLP \n",
    "* ``AttentiveAggregation``: groups-wise attention layer applied to molecular fingerprints multiplied by their individual compositions (meaning that the weighting of the individual fingerprints is adjusted by attention logits)\n",
    "* ``Set2SetAggregation``: recurrent architecture based on LSTMs that aggregate group-wise moleuclar fingerprints multiplied by their individual composition into a mixture representation\n",
    "\n",
    "*Note*: All mixture aggregations operate group-wise and then concatenate the group-based molecular/mixture representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureAggregation(nn.Module, HasHParams):\n",
    "    output_dim: int\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        graph_agg: Aggregation, \n",
    "        groups: Sequence[Sequence[int]], \n",
    "        fp_dims: Sequence[int], \n",
    "        mixmp: MixtureMessagePassing | None, \n",
    "        *args, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hparams = {\n",
    "            \"cls\": self.__class__,\n",
    "            \"groups\": groups,\n",
    "            \"fp_dims\": fp_dims,\n",
    "            \"graph_agg\": graph_agg.hparams,\n",
    "            \"mixmp\": mixmp.hparams if not (mixmp is None) else None\n",
    "        }\n",
    "        self.graph_agg = graph_agg\n",
    "        self.groups = groups\n",
    "        self.fp_dims = fp_dims\n",
    "        self.mixmp = mixmp\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Aggregate component representations into a mixture representation\"\"\"\n",
    "        # Atom-to-molecule aggregation\n",
    "        self.Hs, self.w_fps, self.Hs_batch = self.mol_forward(H_vs, bmgs)\n",
    "        \n",
    "        # Mixture-level message passing\n",
    "        if not (self.mixmp is None):\n",
    "            if isinstance(self.mixmp, MixtureMessagePassing):\n",
    "                # Varying number of components can cause sparse batches, hence complete with zero-entries\n",
    "                self.Hs, self.w_fps, self.Hs_batch = self.complete_sparse_batch(self.Hs, self.w_fps, self.Hs_batch)\n",
    "                self.Hs = self.mixmp(self.Hs)\n",
    "            elif isinstance(self.mixmp, (MolecularMessagePassing, InteractionMessagePassing)):\n",
    "                # Note that we assume MixtureGraphDataset to have index -1 -> TODO: make this dynamic\n",
    "                self.Hs = self.mixmp(bmgs[-1], self.Hs, self.Hs_batch)\n",
    "                # Varying number of components can cause sparse batches, hence complete with zero-entries\n",
    "                self.Hs, self.w_fps, self.Hs_batch = self.complete_sparse_batch(self.Hs, self.w_fps, self.Hs_batch)\n",
    "            else:\n",
    "                raise NotImplementedError(f\":class:`MixtureMessagePassing` of type {type(self.mixmp)} not implemented yet.\")\n",
    "        else:\n",
    "            self.Hs, self.w_fps, self.Hs_batch = self.complete_sparse_batch(self.Hs, self.w_fps, self.Hs_batch)\n",
    "        \n",
    "        # Molecule-to-mixture aggregation: tbi in subclasses\n",
    "        \n",
    "    def mol_forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> tuple[list[Tensor], list[Tensor], list[Tensor]]:\n",
    "        # Hs: n x b x d (but not each mixture has n components, so n x b can be incomplete, hence we need to synthetically adapt the sizes)\n",
    "        Hs, w_fps, Hs_batch = zip(*[(self.graph_agg(H_v, torch.unique(bmg.batch, return_inverse=True)[1]), bmg.w_fps, torch.unique(bmg.batch)) if (isinstance(bmg, BatchComponentMolGraph) and (bmg.batch is not None)) else (self.graph_agg(H_v, torch.unique(bmg.batch, return_inverse=True)[1]), None, torch.unique(bmg.batch)) if (bmg.batch is not None) else (None, None, None) for H_v, bmg in zip(H_vs, bmgs)])\n",
    "        Hs, w_fps, Hs_batch =  list(Hs), list(w_fps), list(Hs_batch)\n",
    "        return Hs, w_fps, Hs_batch\n",
    "\n",
    "    def complete_sparse_batch(\n",
    "        self, Hs: list[Tensor], w_fps: list[Tensor], Hs_batch: list[Tensor]\n",
    "    ) -> tuple[list[Tensor], list[Tensor], list[Tensor]]:\n",
    "        # make Hs and w_fps the same size wrt n x b by adding zero-values/tensors\n",
    "        Hs = self._complete_sparse_tensorlist(Hs, Hs_batch, self.fp_dims)\n",
    "        if not all(f is None for f in w_fps):\n",
    "            w_fps = self._complete_sparse_tensorlist(w_fps, Hs_batch, [None for _ in range(len(Hs_batch))])\n",
    "        return Hs, w_fps, Hs_batch\n",
    "\n",
    "    def _complete_sparse_tensorlist(\n",
    "        self, Hs: list[Tensor], Hs_batch: list[Tensor], dim: list[int]\n",
    "    ) -> list[Tensor]:\n",
    "        batch_size = max(H_b.max().item() for H_b in Hs_batch if not (H_b is None))\n",
    "        device = [H.device for H in Hs if not (H is None)][0] # workaround, TODO\n",
    "        compl_Hs = []\n",
    "        for n_idx, (n_H, n_H_batch) in enumerate(zip(Hs, Hs_batch)):\n",
    "            if dim[n_idx]:\n",
    "                compl_H = torch.zeros((batch_size+1, dim[n_idx]), dtype=torch.float32, device=device)\n",
    "            else:\n",
    "                compl_H = torch.zeros((batch_size+1), dtype=torch.float32, device=device)\n",
    "            if (n_H is not None) and (n_H_batch is not None):\n",
    "                compl_H[n_H_batch] = n_H\n",
    "            compl_Hs.append(compl_H)\n",
    "        return compl_Hs\n",
    "\n",
    "class ConcatAggregation(MixtureAggregation):\n",
    "    r\"\"\"Concatenate aggregation of the graph-level representation:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf h = \\text{concat}_c \\mathbf h_c \n",
    "    \"\"\"\n",
    "    @property\n",
    "    def components_in_mixture(self) -> set[int]:\n",
    "        return {idx for group in self.groups if len(group) > 1 for idx in group}\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims) + len(self.components_in_mixture)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        super().forward(H_vs, bmgs)\n",
    "\n",
    "        w_fps = torch.stack([self.w_fps[idx] for idx in self.components_in_mixture], dim=1)\n",
    "        return torch.cat(self.Hs + [w_fps], 1)\n",
    "\n",
    "class WeightedSumAggregation(MixtureAggregation):\n",
    "    r\"\"\"Weighted sum (MolPool) aggregation of the graph-level representation:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf h = \\sum_{c \\in C} w_{FP} \\mathbf h_c \n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims[group[0]] for group in self.groups)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        super().forward(H_vs, bmgs)\n",
    "\n",
    "        combined_Hs = []\n",
    "        for group in self.groups:\n",
    "            if len(group) == 1:\n",
    "                combined_Hs.append(self.Hs[group[0]])\n",
    "                continue\n",
    "            group_Hs = torch.stack([self.Hs[idx] for idx in group])  # n x b x d\n",
    "            group_w_fps = torch.stack([self.w_fps[idx] for idx in group])  # n x b\n",
    "            # n: num. components in group, b: num. comp. in batch, d: output dim of message passing\n",
    "            combined_H = torch.einsum(\"nb,nbd->bd\", group_w_fps, group_Hs)\n",
    "            combined_Hs.append(combined_H)\n",
    "        return torch.cat(combined_Hs, 1)\n",
    "\n",
    "class DeepsetsAggregation(MixtureAggregation):\n",
    "    r\"\"\"Deep sets aggregation of the graph-level representation:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf h = \\mathrm{MLP_{g}}(\\sum_{c \\in C} \\mathrm{MLP_{l}}(\\mathbf h_c))\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, graph_agg: Aggregation, groups: Sequence[Sequence[int]], fp_dims: Sequence[int], \n",
    "        mixmp: MixtureMessagePassing | None, *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(graph_agg, groups, fp_dims, mixmp, *args, **kwargs)\n",
    "        \n",
    "        self.MLPs_local = nn.ModuleList([])\n",
    "        self.MLPs_global = nn.ModuleList([])\n",
    "        for group in groups:\n",
    "            # TODO: allow to set hparams for MLP by kwargs (e.g., hidden_dim, n_layers)\n",
    "            hidden_dim = self.fp_dims[group[0]]\n",
    "            if len(group) > 1:\n",
    "                self.MLPs_local.append(\n",
    "                    nn.Sequential(\n",
    "                    nn.Linear(self.fp_dims[group[0]], hidden_dim, bias=False),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, hidden_dim, bias=False),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, self.fp_dims[group[0]], bias=False),\n",
    "                    )\n",
    "                )\n",
    "            # MLP global is only used for groups with more than 1 component\n",
    "            self.MLPs_global.append(\n",
    "                nn.Sequential(\n",
    "                nn.Linear(self.fp_dims[group[0]], hidden_dim, bias=False),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim, bias=False),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, self.fp_dims[group[0]], bias=False),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims[group[0]] for group in self.groups)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        super().forward(H_vs, bmgs)\n",
    "\n",
    "        combined_Hs = []\n",
    "        group_len_greater1_counter = 0 # count groups with len > 1 for local layers\n",
    "        for g_idx, group in enumerate(self.groups):\n",
    "            # use only global MLP if group only has one component, as local MLP would just be nested into global MLP\n",
    "            if len(group) == 1:\n",
    "                combined_Hs.append(self.MLPs_global[g_idx](self.Hs[group[0]]))\n",
    "                continue\n",
    "            group_w_Hs = torch.stack([self.MLPs_local[group_len_greater1_counter](self.w_fps[idx].unsqueeze(1) * self.Hs[idx]) for idx in group])  # n x b x d\n",
    "            combined_H = torch.sum(group_w_Hs, dim=0)\n",
    "            combined_Hs.append(self.MLPs_global[g_idx](combined_H))\n",
    "            group_len_greater1_counter += 1\n",
    "        return torch.cat(combined_Hs, 1)\n",
    "\n",
    "class AttentiveAggregation(MixtureAggregation):\n",
    "    r\"\"\"Attentive aggregation of the graph-level representation:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf h = \\sum_{c \\in C} \\alpha_c \\mathbf h_c\n",
    "\n",
    "        \\alpha_c = \\mathrm{softmax}(\\mathbf h_c)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, graph_agg: Aggregation, groups: Sequence[Sequence[int]], fp_dims: Sequence[int], \n",
    "        mixmp: MixtureMessagePassing | None, *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(graph_agg, groups, fp_dims, mixmp, *args, **kwargs)\n",
    "        \n",
    "        self.Ws_a = nn.ModuleList([\n",
    "            nn.Linear(self.fp_dims[group[0]], 1, bias=False) for group in groups if len(group) > 1\n",
    "            ])\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims[group[0]] for group in self.groups)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        super().forward(H_vs, bmgs)\n",
    "\n",
    "        combined_Hs = []\n",
    "        group_len_greater1_counter = 0 # count groups with len > 1 for attentive layers\n",
    "        for _, group in enumerate(self.groups):\n",
    "            if len(group) == 1:\n",
    "                combined_Hs.append((self.Hs[group[0]]))\n",
    "                continue\n",
    "            w_Hs = torch.stack([self.w_fps[idx].unsqueeze(1) * self.Hs[idx] for idx in group])  # n x b x d\n",
    "            attention_logits = self.Ws_a[group_len_greater1_counter](w_Hs).exp().squeeze(2) # n x b\n",
    "            # Ignore logits that correspond to completed zero-tensors due to missing components\n",
    "            # Create a mask tensor\n",
    "            mask = torch.zeros_like(attention_logits, dtype=torch.bool)\n",
    "            # Fill the mask tensor based on index \n",
    "            for tmp_i, idx in enumerate(group):\n",
    "                indices = self.Hs_batch[idx]\n",
    "                if indices is not None:\n",
    "                    mask[tmp_i, indices] = True\n",
    "            # Apply the mask to the original tensor\n",
    "            attention_logits = attention_logits * mask\n",
    "            Z = torch.sum(attention_logits, dim=0, keepdim=True)\n",
    "            alphas = attention_logits / Z\n",
    "            combined_H = torch.sum(alphas.unsqueeze(-1) * w_Hs, dim=0)\n",
    "            combined_Hs.append(combined_H)\n",
    "            group_len_greater1_counter += 1\n",
    "        return torch.cat(combined_Hs, 1)\n",
    "\n",
    "class Set2SetAggregation(MixtureAggregation):\n",
    "    r\"\"\"Set2Set aggregation of the graph-level representation:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{q}_t &= \\mathrm{LSTM}(\\mathbf{q}^{*}_{t-1})\n",
    "\n",
    "        \\alpha_{c,t} &= \\mathrm{softmax}(\\mathbf{h}_c \\cdot \\mathbf{q}_t)\n",
    "\n",
    "        \\mathbf{r}_t &= \\sum_{c=1}^C \\alpha_{c,t} \\mathbf{h}_c\n",
    "\n",
    "        \\mathbf{q}^{*}_t &= \\mathbf{q}_t \\, \\Vert \\, \\mathbf{r}_t,\n",
    "\n",
    "    where :math:`\\mathbf{q}^{*}_T` defines the output of the layer with twice\n",
    "    the dimensionality as the input.\n",
    "    \n",
    "    Note: This implementation follows PyTorch Geometric (cf. https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/aggr/set2set.html#Set2Set) and is based on `\"Order Matters: Sequence to sequence for\n",
    "    Sets\" <https://arxiv.org/abs/1511.06391>`_ paper.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, graph_agg: Aggregation, groups: Sequence[Sequence[int]], fp_dims: Sequence[int], \n",
    "        mixmp: MixtureMessagePassing | None, *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(graph_agg, groups, fp_dims, mixmp, *args, **kwargs)\n",
    "        \n",
    "        # TODO: allow to set hparams for Set2Set by kwargs (e.g., processing steps)\n",
    "        self.processing_steps = 3\n",
    "        self.lstms = nn.ModuleList([])\n",
    "        for group in groups:\n",
    "            if len(group) > 1:\n",
    "                in_channels = self.fp_dims[group[0]]\n",
    "                out_channels = self.fp_dims[group[0]] * 2\n",
    "                self.lstms.append(\n",
    "                    torch.nn.LSTM(out_channels, in_channels, **kwargs)\n",
    "                    )\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims[group[0]] * 2 if len(group) > 1 else self.fp_dims[group[0]] for group in self.groups)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        super().forward(H_vs, bmgs)\n",
    "            \n",
    "        combined_Hs = []\n",
    "        group_len_greater1_counter = 0 # count groups with len > 1 for set2set layers\n",
    "        for _, group in enumerate(self.groups):\n",
    "            if len(group) == 1:\n",
    "                combined_Hs.append((self.Hs[group[0]]))\n",
    "                continue\n",
    "            \n",
    "            w_Hs = torch.stack([self.w_fps[idx].unsqueeze(1) * self.Hs[idx] for idx in group]) \n",
    "            w_Hs = torch.transpose(w_Hs, 0, 1) # b x n x d\n",
    "            b_dim = w_Hs.size(0)\n",
    "            d_dim = w_Hs.size(-1)\n",
    "\n",
    "\n",
    "            h = (w_Hs.new_zeros((self.lstms[group_len_greater1_counter].num_layers, b_dim, d_dim)),\n",
    "                w_Hs.new_zeros((self.lstms[group_len_greater1_counter].num_layers, b_dim, d_dim)))\n",
    "            q_star = w_Hs.new_zeros(b_dim, d_dim * 2)\n",
    "\n",
    "            for _ in range(self.processing_steps):\n",
    "                q, h = self.lstms[group_len_greater1_counter](q_star.unsqueeze(0), h)\n",
    "\n",
    "                q = q.squeeze(0) # b x d\n",
    "                e = torch.sum(w_Hs * q.unsqueeze(1), dim=2) # b x n\n",
    "                attention_logits = e.exp() #.squeeze(2) # b x n\n",
    "\n",
    "                # Ignore logits that correspond to completed zero-tensors due to missing components\n",
    "                # Create a mask tensor\n",
    "                mask = torch.zeros_like(e, dtype=torch.bool)\n",
    "                # Fill the mask tensor based on index tensors\n",
    "                for tmp_i, idx in enumerate(group):\n",
    "                    indices = self.Hs_batch[idx]\n",
    "                    if indices is not None:\n",
    "                        mask[indices, tmp_i] = True\n",
    "                # Apply the mask to the original tensor\n",
    "                attention_logits = attention_logits * mask\n",
    "                Z = torch.sum(attention_logits, dim=1, keepdim=True)\n",
    "                alphas = attention_logits / Z\n",
    "                r = torch.sum(w_Hs * alphas.unsqueeze(2), dim=1) # b x d\n",
    "                q_star = torch.cat([q, r], dim=1) # b x 2*d\n",
    "            combined_Hs.append(q_star)\n",
    "            group_len_greater1_counter += 1\n",
    "        return torch.cat(combined_Hs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPNN model for mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureMPNN(MulticomponentMPNN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_passing: MulticomponentMessagePassing,\n",
    "        agg: Aggregation,\n",
    "        predictor: Predictor,\n",
    "        mix_mpn: MixtureMessagePassing | None = None,\n",
    "        batch_norm: bool = False,\n",
    "        metrics: Iterable[ChempropMetric] | None = None,\n",
    "        warmup_epochs: int = 2,\n",
    "        init_lr: float = 1e-4,\n",
    "        max_lr: float = 1e-3,\n",
    "        final_lr: float = 1e-4,\n",
    "        X_d_transform: ScaleTransform | None = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            message_passing,\n",
    "            agg,\n",
    "            predictor,\n",
    "            batch_norm,\n",
    "            metrics,\n",
    "            warmup_epochs,\n",
    "            init_lr,\n",
    "            max_lr,\n",
    "            final_lr,\n",
    "            X_d_transform,\n",
    "        )\n",
    "        self.agg: MixtureAggregation\n",
    "\n",
    "    def fingerprint(\n",
    "        self,\n",
    "        bmgs: Iterable[BatchComponentMolGraph | BatchMolGraph],\n",
    "        V_ds: Iterable[Tensor],\n",
    "        X_d: Tensor | None = None,\n",
    "    ) -> Tensor:\n",
    "        H_vs: list[Tensor] = self.message_passing(bmgs, V_ds)\n",
    "        H = self.agg(H_vs, bmgs)\n",
    "        H = self.bn(H)\n",
    "        return H if X_d is None else torch.cat((H, X_d), 1)\n",
    "\n",
    "    @classmethod\n",
    "    def _load(cls, path, map_location, **submodules):\n",
    "        d = torch.load(path, map_location, weights_only=False)\n",
    "\n",
    "        try:\n",
    "            hparams = d[\"hyper_parameters\"]\n",
    "            state_dict = d[\"state_dict\"]\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Could not find hyper parameters and/or state dict in {path}.\")\n",
    "\n",
    "        hparams[\"message_passing\"][\"blocks\"] = [\n",
    "            block_hparams.pop(\"cls\")(**block_hparams)\n",
    "            for block_hparams in hparams[\"message_passing\"][\"blocks\"]\n",
    "        ]\n",
    "        graph_agg_hparams = hparams[\"agg\"][\"graph_agg\"]\n",
    "        hparams[\"agg\"][\"graph_agg\"] = graph_agg_hparams.pop(\"cls\")(**graph_agg_hparams)\n",
    "        if not (hparams[\"agg\"][\"mixmp\"] is None):\n",
    "            mixmp_hparams = hparams[\"agg\"][\"mixmp\"]\n",
    "            hparams[\"agg\"][\"mixmp\"] = mixmp_hparams.pop(\"cls\")(**mixmp_hparams)\n",
    "        submodules |= {\n",
    "            key: hparams[key].pop(\"cls\")(**hparams[key])\n",
    "            for key in (\"message_passing\", \"agg\", \"predictor\")\n",
    "            if key not in submodules\n",
    "        }\n",
    "\n",
    "        if not hasattr(submodules[\"predictor\"].criterion, \"_defaults\"):\n",
    "            submodules[\"predictor\"].criterion = submodules[\"predictor\"].criterion.__class__(\n",
    "                task_weights=submodules[\"predictor\"].criterion.task_weights\n",
    "            )\n",
    "\n",
    "        return submodules, state_dict, hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemprop_dir = Path.cwd().parent\n",
    "input_path = (\n",
    "    chemprop_dir / \"tests\" / \"data\" / \"regression\" / \"mol+mol\" / \"mol+mol.csv\"\n",
    ")  # path to your data .csv file containing SMILES strings and target values\n",
    "smiles_columns = [\"smiles\", \"solvent\"]  # name of the column containing SMILES strings\n",
    "target_columns = [\"peakwavs_max\"]  # list of names of the columns containing targets\n",
    "df_input = pd.read_csv(input_path)\n",
    "smiss = df_input.loc[:, smiles_columns].values\n",
    "ys = df_input.loc[:, target_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [[MoleculeDatapoint.from_smi(smis[0], y) for smis, y in zip(smiss, ys)]]\n",
    "all_data += [[ComponentDatapoint.from_smi(smis[0], w_fp=0.1) for smis in smiss]]\n",
    "all_data += [[ComponentDatapoint.from_smi(smis[1], w_fp=0.9) for smis in smiss]]\n",
    "all_data += [[MixtureDatapoint.from_smis(smis) for smis in smiss]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Mixsolv-QM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemprop_dir = Path.cwd().parent\n",
    "input_path = (\n",
    "    chemprop_dir / \"examples\" / \"data\" / \"MixSolvGH-QM.csv\"\n",
    ")  # path to your data .csv file containing SMILES strings and target values\n",
    "smiles_columns = [\"inchi_solute\", \"inchi_solvent1\", \"inchi_solvent2\"]  # name of the column containing SMILES strings\n",
    "frac_columns = [\"frac_solvent1\"]\n",
    "target_columns = [\"Gsolv (kcal/mol)\"]  # list of names of the columns containing targets\n",
    "df_input_sample = pd.read_csv(input_path, sep=\",\").iloc[:100000]\n",
    "smiss = df_input_sample.loc[:, smiles_columns].apply(lambda col: col.apply(lambda x: Chem.MolToSmiles(Chem.MolFromInchi(x)) if x is not np.nan else None)).values\n",
    "fracs = df_input_sample.loc[:, frac_columns]\n",
    "fracs[\"frac_solvent1\"] = fracs[\"frac_solvent1\"].fillna(1.0) # fill in empty molfracs columns with just one component\n",
    "fracs = fracs.values\n",
    "ys = df_input_sample.loc[:, target_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [[MoleculeDatapoint.from_smi(smis[0], y) for smis, y in zip(smiss, ys)]]\n",
    "all_data += [[ComponentDatapoint.from_smi(smis[1], w_fp=f[0]) for smis, f in zip(smiss, fracs)]]\n",
    "all_data += [[ComponentDatapoint.from_smi(smis[2], w_fp=1-f[0]) if smis[2] else ComponentDatapoint(None) for smis, f in zip(smiss, fracs)]]\n",
    "all_data += [[MixtureDatapoint.from_smis(smis) for smis in smiss]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After running either option 1 or 2, continue from here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_to_split_by = 0  # index of the component to use for structure based splits\n",
    "mols = [d.mol for d in all_data[component_to_split_by]]\n",
    "train_indices, val_indices, test_indices = make_split_indices(mols, \"random\", (0.8, 0.1, 0.1))\n",
    "train_data, val_data, test_data = split_data_by_indices(\n",
    "    all_data, train_indices, val_indices, test_indices\n",
    ")\n",
    "train_data = train_data[0]\n",
    "val_data = val_data[0]\n",
    "test_data = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = [\n",
    "    MoleculeDataset(train_data[0]),\n",
    "    ComponentDataset(train_data[1]),\n",
    "    ComponentDataset(train_data[2]),\n",
    "    MixtureGraphDataset(train_data[3]),\n",
    "]\n",
    "val_datasets = [\n",
    "    MoleculeDataset(val_data[0]),\n",
    "    ComponentDataset(val_data[1]),\n",
    "    ComponentDataset(val_data[2]),\n",
    "    MixtureGraphDataset(val_data[3]),\n",
    "]\n",
    "test_datasets = [\n",
    "    MoleculeDataset(test_data[0]),\n",
    "    ComponentDataset(test_data[1]),\n",
    "    ComponentDataset(test_data[2]),\n",
    "    MixtureGraphDataset(test_data[3]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mcdset = MixtureDataset(train_datasets)\n",
    "scaler = train_mcdset.normalize_targets()\n",
    "val_mcdset = MixtureDataset(val_datasets)\n",
    "val_mcdset.normalize_targets(scaler)\n",
    "test_mcdset = MixtureDataset(test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_mcdset, batch_size=100, shuffle=True, collate_fn=collate_mixture)\n",
    "val_loader = DataLoader(val_mcdset, batch_size=100, shuffle=False, collate_fn=collate_mixture)\n",
    "test_loader = DataLoader(test_mcdset, batch_size=100, shuffle=False, collate_fn=collate_mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Molecular message passing\n",
    "mp_depth = 4\n",
    "mp_dh = 200\n",
    "mcmp = MulticomponentMessagePassing(\n",
    "    blocks=[\n",
    "        BondMessagePassing(depth=mp_depth, d_h=mp_dh, activation=\"leakyrelu\"), \n",
    "        BondMessagePassing(depth=mp_depth, d_h=mp_dh, activation=\"leakyrelu\")\n",
    "        ],  \n",
    "        groups=[[0], [1, 2]], \n",
    "        shared=False\n",
    "        )\n",
    "\n",
    "# Mixture message passing\n",
    "DEFAULT_MOL_FDIM , DEFAULT_INTERACTION_FDIM = SimpleMixtureGraphFeaturizer().shape\n",
    "mixmp = InteractionMessagePassing(\n",
    "    depth=3, \n",
    "    d_v=mcmp.blocks[0].output_dim+DEFAULT_MOL_FDIM, \n",
    "    d_h=mcmp.blocks[0].output_dim, \n",
    "    d_e=DEFAULT_INTERACTION_FDIM,\n",
    "    activation=\"leakyrelu\"\n",
    "    )\n",
    "\n",
    "# Atom-to-molecule aggregation\n",
    "graph_agg = MeanAggregation()\n",
    "# Molecule-to-mixture aggregation\n",
    "use_mixmp = True\n",
    "if not use_mixmp:\n",
    "    mixmp = None\n",
    "name_agg = \"weightedsum\"\n",
    "\n",
    "match name_agg:\n",
    "    case \"weightedsum\":\n",
    "        mixagg = WeightedSumAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case \"cat\":\n",
    "        mixagg = ConcatAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case \"deepsets\":\n",
    "        mixagg = DeepsetsAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case \"attentive\":\n",
    "        mixagg = AttentiveAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case \"set2set\":\n",
    "        mixagg = Set2SetAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case _:\n",
    "        raise ValueError(f\"MixtureAggregation {name_agg} not implemented yet.\")\n",
    "\n",
    "# Regression head\n",
    "output_transform = UnscaleTransform.from_standard_scaler(scaler)\n",
    "ffn_activation = \"leakyrelu\"\n",
    "ffn = RegressionFFN(input_dim=mixagg.output_dim, output_transform=output_transform, \n",
    "        hidden_dim=500,\n",
    "        n_layers=4,\n",
    "        activation=ffn_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmpnn = MixtureMPNN(mcmp, mixagg, ffn)\n",
    "mcmpnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=False,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(mcmpnn, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.test(mcmpnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.predict(mcmpnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader.dataset.datasets[0].Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from dataclasses import InitVar, dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Iterable, NamedTuple, Sequence, TypeAlias\n",
    "\n",
    "from lightning import pytorch as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit.Chem import Mol\n",
    "import rdkit.Chem as Chem\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from chemprop.data.collate import BatchMolGraph, collate_batch\n",
    "from chemprop.data.datapoints import MoleculeDatapoint\n",
    "from chemprop.data.datasets import Datum, MoleculeDataset, MulticomponentDataset, ReactionDataset\n",
    "from chemprop.data.molgraph import MolGraph\n",
    "from chemprop.data.splitting import make_split_indices, split_data_by_indices\n",
    "from chemprop.featurizers import Featurizer, SimpleMoleculeMolGraphFeaturizer\n",
    "from chemprop.models import MulticomponentMPNN, multi\n",
    "from chemprop.nn.agg import Aggregation, MeanAggregation\n",
    "from chemprop.nn.hparams import HasHParams\n",
    "from chemprop.nn.message_passing import BondMessagePassing, MessagePassing\n",
    "from chemprop.nn.metrics import ChempropMetric, MSE, RMSE, MAE, R2Score\n",
    "from chemprop.nn.predictors import Predictor, RegressionFFN\n",
    "from chemprop.nn.transforms import ScaleTransform, UnscaleTransform\n",
    "from chemprop.nn.utils import Activation, get_activation_function\n",
    "from lightning.pytorch.core.mixins import HyperparametersMixin\n",
    "from chemprop.conf import DEFAULT_ATOM_FDIM, DEFAULT_BOND_FDIM, DEFAULT_HIDDEN_DIM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extend the `MolGraph` class to include a global attribute `w_fp` which is the weight of the learned fingerprint of the molecule when averaging the fingerprints of components in the mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also chemprop.data.molgraph.MolGraph\n",
    "class ComponentMolGraph(NamedTuple):\n",
    "    V: np.ndarray\n",
    "    E: np.ndarray\n",
    "    edge_index: np.ndarray\n",
    "    rev_edge_index: np.ndarray\n",
    "    w_fp: float = 1.0\n",
    "    \"\"\"the weight of the component's fingerprint when combining components in the mixture\"\"\"\n",
    "\n",
    "\n",
    "# See also chemprop.data.datasets.Datum\n",
    "class ComponentDatum(NamedTuple):\n",
    "    mg: ComponentMolGraph\n",
    "    V_d: np.ndarray | None\n",
    "    x_d: np.ndarray | None\n",
    "    y: np.ndarray | None\n",
    "    weight: float\n",
    "    lt_mask: np.ndarray | None\n",
    "    gt_mask: np.ndarray | None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batched versions of `MolGraph` and `Datum` are created during collating datapoints. These are also extended, as well as the entire batch representing all components in the mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also chemprop.data.collate.BatchMolGraph\n",
    "@dataclass(repr=False, eq=False, slots=True)\n",
    "class BatchComponentMolGraph(BatchMolGraph):\n",
    "    mgs: InitVar[Sequence[ComponentMolGraph]]\n",
    "    w_fps: Tensor = field(init=False)\n",
    "    __is_empty: bool = field(init=False)\n",
    "\n",
    "    def __post_init__(self, mgs):    \n",
    "        # super(BatchComponentMolGraph, self).__post_init__(mgs) \n",
    "        self._BatchMolGraph__size = len(mgs)\n",
    "        self.__is_empty = True   \n",
    "        Vs = []\n",
    "        Es = []\n",
    "        edge_indexes = []\n",
    "        rev_edge_indexes = []\n",
    "        batch_indexes = []\n",
    "        w_fps = []\n",
    "\n",
    "        num_nodes = 0\n",
    "        num_edges = 0\n",
    "        for i, mg in enumerate(mgs):\n",
    "            if mg is None:\n",
    "                continue\n",
    "            Vs.append(mg.V)\n",
    "            Es.append(mg.E)\n",
    "            edge_indexes.append(mg.edge_index + num_nodes)\n",
    "            rev_edge_indexes.append(mg.rev_edge_index + num_edges)\n",
    "            batch_indexes.append([i] * len(mg.V))\n",
    "            w_fps.append(mg.w_fp)\n",
    "\n",
    "            num_nodes += mg.V.shape[0]\n",
    "            num_edges += mg.edge_index.shape[1]\n",
    "\n",
    "        self.V = torch.from_numpy(np.concatenate(Vs)).float() if len(Vs) > 0 else None\n",
    "        self.E = torch.from_numpy(np.concatenate(Es)).float() if len(Es) > 0 else None\n",
    "        self.edge_index = torch.from_numpy(np.hstack(edge_indexes)).long() if len(edge_indexes) > 0 else None\n",
    "        self.rev_edge_index = torch.from_numpy(np.concatenate(rev_edge_indexes)).long() if len(rev_edge_indexes) > 0 else None\n",
    "        self.batch = torch.tensor(np.concatenate(batch_indexes)).long() if len(batch_indexes) > 0 else None\n",
    "        self.w_fps = torch.from_numpy(np.array(w_fps)).float() if len(w_fps) > 0 else None\n",
    "\n",
    "        if len(Vs) > 0:\n",
    "            self.__is_empty = False\n",
    "\n",
    "    def to(self, device: str | torch.device):\n",
    "        if not self.is_empty():\n",
    "            super(BatchComponentMolGraph, self).to(device)\n",
    "            self.w_fps = self.w_fps.to(device)\n",
    "\n",
    "    def is_empty(self) -> bool:\n",
    "        \"\"\"whether any :class:`MolGraph`\\s are stored in this batch\"\"\"\n",
    "        return self.__is_empty\n",
    "\n",
    "\n",
    "# See also chemprop.data.collate.TrainingBatch\n",
    "class BatchComponentDatum(NamedTuple):\n",
    "    bmg: BatchComponentMolGraph\n",
    "    V_d: Tensor | None\n",
    "    X_d: Tensor | None\n",
    "    Y: Tensor | None\n",
    "    w: Tensor\n",
    "    lt_mask: Tensor | None\n",
    "    gt_mask: Tensor | None\n",
    "\n",
    "\n",
    "# See also chemprop.data.collate.MulticomponentTrainingBatch\n",
    "class MixtureBatch(NamedTuple):\n",
    "    bmgs: list[BatchMolGraph | BatchComponentMolGraph]\n",
    "    V_ds: list[Tensor | None]\n",
    "    X_d: Tensor | None\n",
    "    Y: Tensor | None\n",
    "    w: Tensor\n",
    "    lt_mask: Tensor | None\n",
    "    gt_mask: Tensor | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See also chemprop.data.collate.collate_batch\n",
    "def collate_component(batch: Iterable[Datum]) -> BatchComponentDatum:\n",
    "    mgs, V_ds, x_ds, ys, weights, lt_masks, gt_masks = zip(*batch)\n",
    "\n",
    "    return BatchComponentDatum(\n",
    "        BatchComponentMolGraph(mgs),\n",
    "        None if V_ds[0] is None else torch.from_numpy(np.concatenate(V_ds)).float(),\n",
    "        None if x_ds[0] is None else torch.from_numpy(np.array(x_ds)).float(),\n",
    "        None if ys[0] is None else torch.from_numpy(np.array(ys)).float(),\n",
    "        torch.tensor(weights, dtype=torch.float).unsqueeze(1),\n",
    "        None if lt_masks[0] is None else torch.from_numpy(np.array(lt_masks)),\n",
    "        None if gt_masks[0] is None else torch.from_numpy(np.array(gt_masks)),\n",
    "    )\n",
    "\n",
    "\n",
    "# See also chemprop.data.collate.collate_multicomponent\n",
    "def collate_mixture(batches: Iterable[Iterable[ComponentDatum | Datum]]) -> MixtureBatch:\n",
    "    tbs = [\n",
    "        collate_batch(batch) if isinstance(batch[0], Datum) else collate_component(batch)\n",
    "        for batch in zip(*batches)\n",
    "    ]\n",
    "\n",
    "    return MixtureBatch(\n",
    "        [tb.bmg for tb in tbs],\n",
    "        [tb.V_d for tb in tbs],\n",
    "        tbs[0].X_d,\n",
    "        tbs[0].Y,\n",
    "        tbs[0].w,\n",
    "        tbs[0].lt_mask,\n",
    "        tbs[0].gt_mask,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ComponentDatapoint(MoleculeDatapoint):\n",
    "    w_fp: np.ndarray | None = None\n",
    "    \"\"\"the weight of the molecule's learned fingerprint when averaging in the mixture\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ComponentDataset(MoleculeDataset, Dataset[ComponentMolGraph]):\n",
    "    data: list[ComponentDatapoint]\n",
    "\n",
    "    @property\n",
    "    def w_fps(self) -> np.ndarray:\n",
    "        return np.array([d.w_fp for d in self.data])\n",
    "\n",
    "    def __getitem__(self, idx: int) -> ComponentDatum:\n",
    "        d = self.data[idx]\n",
    "        if d.mol:\n",
    "            mg = self.mg_cache[idx] \n",
    "            mg = ComponentMolGraph(w_fp=d.w_fp, *mg)\n",
    "        else:\n",
    "            mg = None\n",
    "\n",
    "        return ComponentDatum(\n",
    "            mg, self.V_ds[idx], self.X_d[idx], self.Y[idx], d.weight, d.lt_mask, d.gt_mask\n",
    "        )\n",
    "\n",
    "@dataclass(repr=False, eq=False)\n",
    "class MixtureDataset(MulticomponentDataset):\n",
    "    datasets: list[MoleculeDataset | ReactionDataset | ComponentDataset]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> list[ComponentDatum | Datum]:\n",
    "        return [dset[idx] for dset in self.datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticomponentMessagePassing(nn.Module, HasHParams):\n",
    "    \"\"\"A `MulticomponentMessagePassing` performs message-passing on each individual input in a\n",
    "    multicomponent input then concatenates the representation of each input to construct a\n",
    "    global representation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    blocks : Sequence[MessagePassing]\n",
    "        the invidual message-passing blocks for each input\n",
    "    n_components : int\n",
    "        the number of components in each input\n",
    "    shared : bool, default=False\n",
    "        whether one block will be shared among all components in an input. If not, a separate\n",
    "        block will be learned for each component.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        blocks: Sequence[MessagePassing], \n",
    "        groups: Sequence[Sequence[int]], \n",
    "        shared: bool = False,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.hparams = {\n",
    "            \"cls\": self.__class__,\n",
    "            \"blocks\": [block.hparams for block in blocks],\n",
    "            \"groups\": groups,\n",
    "            \"shared\": shared,\n",
    "        }\n",
    "\n",
    "        if len(blocks) == 0:\n",
    "            raise ValueError(\"arg 'blocks' was empty!\")\n",
    "        if groups is None:\n",
    "            raise ValueError(\"arg 'groups' was empty!\")\n",
    "\n",
    "        if shared:\n",
    "            if len(blocks) > 1:\n",
    "                logger.warning(\n",
    "                    \"More than 1 block was supplied but 'shared' was True! Using only the 0th block...\"\n",
    "                )\n",
    "            if len(groups) != sum(len(g) if isinstance(g, list) else 1 for g in groups):\n",
    "                logger.warning(\n",
    "                    \"Different groups were supplied but 'shared' was True! Using only the 0th block for all groups...\"\n",
    "                )\n",
    "        else:\n",
    "            if len(blocks) != len(groups):\n",
    "                raise ValueError(\n",
    "                    \"arg 'len(groups)' must be equal to `len(blocks)` if 'shared' is False!\"\n",
    "                    f\"got: {len(groups)} and {len(blocks)}, respectively.\"\n",
    "                )\n",
    "\n",
    "        self.groups = groups\n",
    "        self.shared = shared\n",
    "        self.blocks = nn.ModuleList()\n",
    "        if shared:\n",
    "            self.blocks.extend([blocks[0]] * len(groups))\n",
    "        else:\n",
    "            b_idx = 0\n",
    "            for g_idx, g in enumerate(groups):\n",
    "                self.blocks.extend([blocks[g_idx]] * len(g))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.blocks)\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        d_o = sum(block.output_dim for block in self.blocks)\n",
    "\n",
    "        return d_o\n",
    "\n",
    "    def forward(self, bmgs: Iterable[BatchMolGraph], V_ds: Iterable[Tensor | None]) -> list[Tensor]:\n",
    "        \"\"\"Encode the multicomponent inputs\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bmgs : Iterable[BatchMolGraph]\n",
    "        V_ds : Iterable[Tensor | None]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[Tensor]\n",
    "            a list of tensors of shape `V x d_i` containing the respective encodings of the `i`\\th\n",
    "            component, where `d_i` is the output dimension of the `i`\\th encoder\n",
    "        \"\"\"\n",
    "        if V_ds is None:\n",
    "            return [block(bmg) for block, bmg in zip(self.blocks, bmgs)]\n",
    "        else:\n",
    "            return [block(bmg, V_d) for block, bmg, V_d in zip(self.blocks, bmgs, V_ds)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureMessagePassing(nn.Module, HyperparametersMixin):\n",
    "    r\"\"\"A :class:`MixtureMessagePassing` encodes a batch of mixtures by passing messages along\n",
    "    molecules constructing a fully connected graph to model intermolecular interactions.\n",
    "\n",
    "    It implements the following operation:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        h_v^{(0)} &= \\tau \\left( \\mathbf{W}_i(x_v) \\right) \\\\\n",
    "        m_v^{(t)} &= \\sum_{u \\in \\mathcal{w \\in V \\setminu v} h_w^{(t-1)} \\\\\n",
    "        h_v^{(T)} &= \\tau\\left(h_v^{(0)} + \\mathbf{W}_h m_v^{(t-1)}\\right) \\\\\n",
    "\n",
    "    where :math:`\\tau` is the activation function; :math:`\\mathbf{W}_i`, :math:`\\mathbf{W}_h` are learned weight matrices; :math:`e_{vw}` is the feature vector of the\n",
    "    bond between molecules :math:`v` and :math:`w`; :math:`x_v` is the feature vector of molecule :math:`v`;\n",
    "    :math:`h_v^{(t)}` is the hidden representation of atom :math:`v` at iteration :math:`t`;\n",
    "    :math:`m_v^{(t)}` is the message received by atom :math:`v` at iteration :math:`t`; and\n",
    "    :math:`t \\in \\{1, \\dots, T\\}` is the number of message passing iterations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_v: int = DEFAULT_HIDDEN_DIM,\n",
    "        d_e: int | None = None,\n",
    "        d_h: int = DEFAULT_HIDDEN_DIM,\n",
    "        d_vd: int | None = None,\n",
    "        bias: bool = False,\n",
    "        depth: int = 1,\n",
    "        activation: str | Activation = Activation.RELU,\n",
    "    ):        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.hparams[\"cls\"] = self.__class__\n",
    "\n",
    "        self.depth = depth\n",
    "        self.tau = get_activation_function(activation)\n",
    "        \n",
    "        self.W_i = nn.Linear(d_v, d_h, bias)\n",
    "        self.W_h = nn.Linear(d_h, d_h, bias) # TODO consider E\n",
    "        self.W_o = None\n",
    "        self.W_d = None\n",
    "\n",
    "    def initialize(self, V: Tensor) -> Tensor:\n",
    "        return self.W_i(V)\n",
    "\n",
    "    def message(self, H: Tensor):\n",
    "        # assume fully connected graph, TODO\n",
    "        H = torch.transpose(H, 0, 1) # b x n x d\n",
    "        M_t = H.unsqueeze(2).expand(-1, -1, H.size(1), -1) # b x n x n x d\n",
    "        M_t = self.W_h(M_t)\n",
    "        mask = ~torch.eye(H.size(1), dtype=bool, device=H.device).unsqueeze(0) # exclude self-loops (n x n)\n",
    "        M_t = (M_t * mask.unsqueeze(-1)).sum(dim=1) # b x n x d\n",
    "        M_t = torch.transpose(M_t, 0, 1) # n x b x d\n",
    "        return M_t\n",
    "\n",
    "    def update(self, M_t: Tensor, H_0: Tensor):\n",
    "        H_t = self.tau(H_0 + M_t)\n",
    "        return H_t\n",
    "\n",
    "    def finalize(self, H_t: Tensor):\n",
    "        return [h for h in H_t]\n",
    "\n",
    "    def forward(self, V: list[Tensor]):\n",
    "        H_0 = self.initialize(torch.stack(V))\n",
    "        H = self.tau(H_0)\n",
    "        for _ in range(self.depth):\n",
    "            M = self.message(H)\n",
    "            H = self.update(M, H_0)\n",
    "\n",
    "        return self.finalize(H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureAggregation(nn.Module, HasHParams):\n",
    "    output_dim: int\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        graph_agg: Aggregation, \n",
    "        groups: Sequence[Sequence[int]], \n",
    "        fp_dims: Sequence[int], \n",
    "        mixmp: MixtureMessagePassing | None, \n",
    "        *args, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hparams = {\n",
    "            \"cls\": self.__class__,\n",
    "            \"groups\": groups,\n",
    "            \"fp_dims\": fp_dims,\n",
    "        }\n",
    "        self.graph_agg = graph_agg\n",
    "        self.groups = groups\n",
    "        self.fp_dims = fp_dims\n",
    "        self.mixmp = mixmp\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(\n",
    "        self, Hs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Aggregate component representations into a mixture representation\"\"\"\n",
    "\n",
    "    def mol_forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> tuple[list[Tensor], list[Tensor], list[Tensor]]:\n",
    "        # Hs: n x b x d (but not each mixture has n components, so n x b can be incomplete, hence we need to synthetically adapt the sizes)\n",
    "        Hs, w_fps, Hs_batch = zip(*[(self.graph_agg(H_v, torch.unique(bmg.batch, return_inverse=True)[1]), bmg.w_fps, torch.unique(bmg.batch)) if (isinstance(bmg, BatchComponentMolGraph) and (bmg.batch is not None)) else (self.graph_agg(H_v, torch.unique(bmg.batch, return_inverse=True)[1]), None, torch.unique(bmg.batch)) if (bmg.batch is not None) else (None, None, None) for H_v, bmg in zip(H_vs, bmgs)])\n",
    "        Hs, w_fps, Hs_batch =  list(Hs), list(w_fps), list(Hs_batch)\n",
    "        return Hs, w_fps, Hs_batch\n",
    "\n",
    "    def complete_sparse_batch(\n",
    "        self, Hs: list[Tensor], w_fps: list[Tensor], Hs_batch: list[Tensor]\n",
    "    ) -> tuple[list[Tensor], list[Tensor], list[Tensor]]:\n",
    "        # make Hs and w_fps the same size wrt n x b by adding zero-values/tensors\n",
    "        Hs = self._complete_sparse_tensorlist(Hs, Hs_batch, self.fp_dims)\n",
    "        if not all(f is None for f in w_fps):\n",
    "            w_fps = self._complete_sparse_tensorlist(w_fps, Hs_batch, [None for _ in range(len(Hs_batch))])\n",
    "        return Hs, w_fps, Hs_batch\n",
    "\n",
    "    def _complete_sparse_tensorlist(\n",
    "        self, Hs: list[Tensor], Hs_batch: list[Tensor], dim: list[int]\n",
    "    ) -> list[Tensor]:\n",
    "        #dim = max(H_b.a)\n",
    "        batch_size = max(H_b.max().item() for H_b in Hs_batch if not (H_b is None))\n",
    "        device = [H.device for H in Hs if not (H is None)][0] # workaround, TODO\n",
    "        compl_Hs = []\n",
    "        for n_idx, (n_H, n_H_batch) in enumerate(zip(Hs, Hs_batch)):\n",
    "            if dim[n_idx]:\n",
    "                compl_H = torch.zeros((batch_size+1, dim[n_idx]), dtype=torch.float32, device=device)\n",
    "            else:\n",
    "                compl_H = torch.zeros((batch_size+1), dtype=torch.float32, device=device)\n",
    "            if (n_H is not None) and (n_H_batch is not None):\n",
    "                compl_H[n_H_batch] = n_H\n",
    "            compl_Hs.append(compl_H)\n",
    "        return compl_Hs\n",
    "\n",
    "class WeightedSumAggregation(MixtureAggregation):\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims[group[0]] for group in self.groups)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        Hs, w_fps, Hs_batch = self.mol_forward(H_vs, bmgs)\n",
    "        Hs, w_fps, Hs_batch = self.complete_sparse_batch(Hs, w_fps, Hs_batch)\n",
    "\n",
    "        if not (self.mixmp is None):\n",
    "            Hs = self.mixmp(Hs)\n",
    "\n",
    "        combined_Hs = []\n",
    "        for group in self.groups:\n",
    "            if len(group) == 1:\n",
    "                combined_Hs.append(Hs[group[0]])\n",
    "                continue\n",
    "            group_Hs = torch.stack([Hs[idx] for idx in group])  # n x b x d\n",
    "            #import pdb\n",
    "            #pdb.set_trace()\n",
    "            group_w_fps = torch.stack([w_fps[idx] for idx in group])  # n x b\n",
    "            #pdb.set_trace()\n",
    "            # n: num. components in group, b: num. comp. in batch, d: output dim of message passing\n",
    "            combined_H = torch.einsum(\"nb,nbd->bd\", group_w_fps, group_Hs)\n",
    "            combined_Hs.append(combined_H)\n",
    "        #pdb.set_trace()\n",
    "        return torch.cat(combined_Hs, 1)\n",
    "\n",
    "class ConcatAggregation(MixtureAggregation):\n",
    "    @property\n",
    "    def components_in_mixture(self) -> set[int]:\n",
    "        return {idx for group in self.groups if len(group) > 1 for idx in group}\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims) + len(self.components_in_mixture)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        Hs, w_fps, Hs_batch = self.mol_forward(H_vs, bmgs)\n",
    "        Hs, w_fps, Hs_batch = self.complete_sparse_batch(Hs, w_fps, Hs_batch)\n",
    "\n",
    "        if not (self.mixmp is None):\n",
    "            Hs = self.mixmp(Hs)\n",
    "\n",
    "        w_fps = torch.stack([w_fps[idx] for idx in self.components_in_mixture], dim=1)\n",
    "        return torch.cat(Hs + [w_fps], 1)\n",
    "\n",
    "class DeepsetsAggregation(MixtureAggregation):\n",
    "    r\"\"\"Deep sets aggregation of the graph-level representation:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf h = \\mathrm{MLP_{g}}(\\sum_{c \\in C} \\mathrm{MLP_{l}}(\\mathbf h_c))\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, graph_agg: Aggregation, groups: Sequence[Sequence[int]], fp_dims: Sequence[int], \n",
    "        mixmp: MixtureMessagePassing | None, *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(graph_agg, groups, fp_dims, mixmp, *args, **kwargs)\n",
    "        \n",
    "        self.MLPs_local = nn.ModuleList([])\n",
    "        self.MLPs_global = nn.ModuleList([])\n",
    "        for group in groups:\n",
    "            # TODO: allow to set hparams for MLP by kwargs (e.g., hidden_dim, n_layers)\n",
    "            hidden_dim = self.fp_dims[group[0]]\n",
    "            self.MLPs_local.append(\n",
    "                nn.Sequential(\n",
    "                nn.Linear(self.fp_dims[group[0]], hidden_dim, bias=False),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim, bias=False),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, self.fp_dims[group[0]], bias=False),\n",
    "                )\n",
    "            )\n",
    "            self.MLPs_global.append(\n",
    "                nn.Sequential(\n",
    "                nn.Linear(self.fp_dims[group[0]], hidden_dim, bias=False),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim, bias=False),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, self.fp_dims[group[0]], bias=False),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims[group[0]] for group in self.groups)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        Hs, w_fps, Hs_batch = self.mol_forward(H_vs, bmgs)\n",
    "        Hs, w_fps, Hs_batch = self.complete_sparse_batch(Hs, w_fps, Hs_batch)\n",
    "        \n",
    "        if not (self.mixmp is None):\n",
    "            Hs = self.mixmp(Hs)\n",
    "\n",
    "        combined_Hs = []\n",
    "        for g_idx, group in enumerate(self.groups):\n",
    "            # use only global MLP if group only has one component, as local MLP would just be nested into global MLP\n",
    "            if len(group) == 1:\n",
    "                combined_Hs.append(self.MLPs_global[g_idx](Hs[group[0]]))\n",
    "                continue\n",
    "            group_w_Hs = torch.stack([self.MLPs_local[g_idx](w_fps[idx].unsqueeze(1) * Hs[idx]) for idx in group])  # n x b x d\n",
    "            combined_H = torch.sum(group_w_Hs, dim=0)\n",
    "            combined_Hs.append(self.MLPs_global[g_idx](combined_H))\n",
    "        return torch.cat(combined_Hs, 1)\n",
    "\n",
    "class AttentiveAggregation(MixtureAggregation):\n",
    "    r\"\"\"Attentive aggregation of the graph-level representation:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf h = \\sum_{c \\in C} \\alpha_c \\mathbf h_c\n",
    "\n",
    "        \\alpha_c = \\mathrm{softmax}(\\mathbf h_c)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, graph_agg: Aggregation, groups: Sequence[Sequence[int]], fp_dims: Sequence[int], \n",
    "        mixmp: MixtureMessagePassing | None, *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(graph_agg, groups, fp_dims, mixmp, *args, **kwargs)\n",
    "        \n",
    "        self.Ws_a = nn.ModuleList([\n",
    "            nn.Linear(self.fp_dims[group[0]], 1, bias=False) for group in groups\n",
    "            ])\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims[group[0]] for group in self.groups)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        Hs, w_fps, Hs_batch = self.mol_forward(H_vs, bmgs)\n",
    "        Hs, w_fps, Hs_batch = self.complete_sparse_batch(Hs, w_fps, Hs_batch)\n",
    "\n",
    "        if not (self.mixmp is None):\n",
    "            Hs = self.mixmp(Hs)\n",
    "\n",
    "        combined_Hs = []\n",
    "        for g_idx, group in enumerate(self.groups):\n",
    "            if len(group) == 1:\n",
    "                combined_Hs.append((Hs[group[0]]))\n",
    "                continue\n",
    "            w_Hs = torch.stack([w_fps[idx].unsqueeze(1) * Hs[idx] for idx in group])  # n x b x d\n",
    "            attention_logits = self.Ws_a[g_idx](w_Hs).exp().squeeze(2) # n x b\n",
    "            # Ignore logits that correspond to completed zero-tensors due to missing components\n",
    "            # Create a mask tensor\n",
    "            mask = torch.zeros_like(attention_logits, dtype=torch.bool)\n",
    "            # Fill the mask tensor based on index \n",
    "            for tmp_i, idx in enumerate(group):\n",
    "                indices = Hs_batch[idx]\n",
    "                if indices is not None:\n",
    "                    mask[tmp_i, indices] = True\n",
    "            # Apply the mask to the original tensor\n",
    "            attention_logits = attention_logits * mask\n",
    "            #attention_logits = torch.stack(self.complete_sparse_batch([a.unsqueeze(0) for a in attention_logits], Hs_batch, [None for _ in attention_logits]))\n",
    "            Z = torch.sum(attention_logits, dim=0, keepdim=True)\n",
    "            alphas = attention_logits / Z\n",
    "            combined_H = torch.sum(alphas.unsqueeze(-1) * w_Hs, dim=0)\n",
    "            #import pdb\n",
    "            combined_Hs.append(combined_H)\n",
    "        return torch.cat(combined_Hs, 1)\n",
    "\n",
    "class Set2SetAggregation(MixtureAggregation):\n",
    "    r\"\"\"Set2Set aggregation of the graph-level representation:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{q}_t &= \\mathrm{LSTM}(\\mathbf{q}^{*}_{t-1})\n",
    "\n",
    "        \\alpha_{c,t} &= \\mathrm{softmax}(\\mathbf{h}_c \\cdot \\mathbf{q}_t)\n",
    "\n",
    "        \\mathbf{r}_t &= \\sum_{c=1}^C \\alpha_{c,t} \\mathbf{h}_c\n",
    "\n",
    "        \\mathbf{q}^{*}_t &= \\mathbf{q}_t \\, \\Vert \\, \\mathbf{r}_t,\n",
    "\n",
    "    where :math:`\\mathbf{q}^{*}_T` defines the output of the layer with twice\n",
    "    the dimensionality as the input.\n",
    "    \n",
    "    Note: This implementation follows PyTorch Geometric (cf. https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/aggr/set2set.html#Set2Set) and is based on `\"Order Matters: Sequence to sequence for\n",
    "    Sets\" <https://arxiv.org/abs/1511.06391>`_ paper.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, graph_agg: Aggregation, groups: Sequence[Sequence[int]], fp_dims: Sequence[int], \n",
    "        mixmp: MixtureMessagePassing | None, *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(graph_agg, groups, fp_dims, mixmp, *args, **kwargs)\n",
    "        \n",
    "        # TODO: allow to set hparams for Set2Set by kwargs (e.g., processing steps)\n",
    "        self.processing_steps = 3\n",
    "        self.lstms = nn.ModuleList([])\n",
    "        for group in groups:\n",
    "            in_channels = self.fp_dims[group[0]]\n",
    "            out_channels = self.fp_dims[group[0]] * 2\n",
    "            self.lstms.append(\n",
    "                torch.nn.LSTM(out_channels, in_channels, **kwargs)\n",
    "                )\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return sum(self.fp_dims[group[0]] * 2 if len(group) > 1 else self.fp_dims[group[0]] for group in self.groups)\n",
    "\n",
    "    def forward(\n",
    "        self, H_vs: list[Tensor], bmgs: list[BatchComponentMolGraph | BatchMolGraph]\n",
    "    ) -> Tensor:\n",
    "        Hs, w_fps, Hs_batch = self.mol_forward(H_vs, bmgs)\n",
    "        Hs, w_fps, Hs_batch = self.complete_sparse_batch(Hs, w_fps, Hs_batch)\n",
    "\n",
    "        if not (self.mixmp is None):\n",
    "            Hs = self.mixmp(Hs)\n",
    "            \n",
    "        combined_Hs = []\n",
    "        for g_idx, group in enumerate(self.groups):\n",
    "            if len(group) == 1:\n",
    "                combined_Hs.append((Hs[group[0]]))\n",
    "                continue\n",
    "            \n",
    "            w_Hs = torch.stack([w_fps[idx].unsqueeze(1) * Hs[idx] for idx in group]) \n",
    "            w_Hs = torch.transpose(w_Hs, 0, 1) # b x n x d\n",
    "            b_dim = w_Hs.size(0)\n",
    "            d_dim = w_Hs.size(-1)\n",
    "\n",
    "            h = (w_Hs.new_zeros((self.lstms[g_idx].num_layers, b_dim, d_dim)),\n",
    "                w_Hs.new_zeros((self.lstms[g_idx].num_layers, b_dim, d_dim)))\n",
    "            q_star = w_Hs.new_zeros(b_dim, d_dim * 2)\n",
    "\n",
    "            for _ in range(self.processing_steps):\n",
    "                q, h = self.lstms[g_idx](q_star.unsqueeze(0), h)\n",
    "\n",
    "                q = q.squeeze(0) # b x d\n",
    "                e = torch.sum(w_Hs * q.unsqueeze(1), dim=2) # b x n\n",
    "                \n",
    "                #a = torch.softmax(e, dim=1) # b x n, old version\n",
    "\n",
    "                attention_logits = e.exp() #.squeeze(2) # b x n\n",
    "\n",
    "                # Ignore logits that correspond to completed zero-tensors due to missing components\n",
    "                # Create a mask tensor\n",
    "                mask = torch.zeros_like(e, dtype=torch.bool)\n",
    "                # Fill the mask tensor based on index tensors\n",
    "                for tmp_i, idx in enumerate(group):\n",
    "                    indices = Hs_batch[idx]\n",
    "                    if indices is not None:\n",
    "                        mask[indices, tmp_i] = True\n",
    "                # Apply the mask to the original tensor\n",
    "                attention_logits = attention_logits * mask\n",
    "                Z = torch.sum(attention_logits, dim=1, keepdim=True)\n",
    "                alphas = attention_logits / Z\n",
    "                r = torch.sum(w_Hs * alphas.unsqueeze(2), dim=1) # b x d\n",
    "                q_star = torch.cat([q, r], dim=1) # b x 2*d\n",
    "\n",
    "            combined_Hs.append(q_star)\n",
    "        return torch.cat(combined_Hs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureMPNN(MulticomponentMPNN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_passing: MulticomponentMessagePassing,\n",
    "        agg: Aggregation,\n",
    "        predictor: Predictor,\n",
    "        mix_mpn: MixtureMessagePassing | None = None,\n",
    "        batch_norm: bool = False,\n",
    "        metrics: Iterable[ChempropMetric] | None = None,\n",
    "        warmup_epochs: int = 2,\n",
    "        init_lr: float = 1e-4,\n",
    "        max_lr: float = 1e-3,\n",
    "        final_lr: float = 1e-4,\n",
    "        X_d_transform: ScaleTransform | None = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            message_passing,\n",
    "            agg,\n",
    "            predictor,\n",
    "            batch_norm,\n",
    "            metrics,\n",
    "            warmup_epochs,\n",
    "            init_lr,\n",
    "            max_lr,\n",
    "            final_lr,\n",
    "            X_d_transform,\n",
    "        )\n",
    "        self.agg: MixtureAggregation\n",
    "\n",
    "    def fingerprint(\n",
    "        self,\n",
    "        bmgs: Iterable[BatchComponentMolGraph | BatchMolGraph],\n",
    "        V_ds: Iterable[Tensor],\n",
    "        X_d: Tensor | None = None,\n",
    "    ) -> Tensor:\n",
    "        H_vs: list[Tensor] = self.message_passing(bmgs, V_ds)\n",
    "        H = self.agg(H_vs, bmgs)\n",
    "        H = self.bn(H)\n",
    "        return H if X_d is None else torch.cat((H, X_d), 1)\n",
    "\n",
    "    @classmethod\n",
    "    def _load(cls, path, map_location, **submodules):\n",
    "        d = torch.load(path, map_location, weights_only=False)\n",
    "\n",
    "        try:\n",
    "            hparams = d[\"hyper_parameters\"]\n",
    "            state_dict = d[\"state_dict\"]\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"Could not find hyper parameters and/or state dict in {path}.\")\n",
    "\n",
    "        hparams[\"message_passing\"][\"blocks\"] = [\n",
    "            block_hparams.pop(\"cls\")(**block_hparams)\n",
    "            for block_hparams in hparams[\"message_passing\"][\"blocks\"]\n",
    "        ]\n",
    "        graph_agg_hparams = hparams[\"agg\"][\"graph_agg\"]\n",
    "        hparams[\"agg\"][\"graph_agg\"] = graph_agg_hparams.pop(\"cls\")(**graph_agg_hparams)\n",
    "        mixmp_hparams = hparams[\"agg\"][\"mixmp\"]\n",
    "        hparams[\"agg\"][\"mixmp\"] = mixmp_hparams.pop(\"cls\")(**mixmp_hparams)\n",
    "        submodules |= {\n",
    "            key: hparams[key].pop(\"cls\")(**hparams[key])\n",
    "            for key in (\"message_passing\", \"agg\", \"predictor\")\n",
    "            if key not in submodules\n",
    "        }\n",
    "\n",
    "        if not hasattr(submodules[\"predictor\"].criterion, \"_defaults\"):\n",
    "            submodules[\"predictor\"].criterion = submodules[\"predictor\"].criterion.__class__(\n",
    "                task_weights=submodules[\"predictor\"].criterion.task_weights\n",
    "            )\n",
    "\n",
    "        return submodules, state_dict, hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemprop_dir = Path.cwd().parent\n",
    "input_path = (\n",
    "    chemprop_dir / \"tests\" / \"data\" / \"regression\" / \"mol+mol\" / \"mol+mol.csv\"\n",
    ")  # path to your data .csv file containing SMILES strings and target values\n",
    "smiles_columns = [\"smiles\", \"solvent\"]  # name of the column containing SMILES strings\n",
    "target_columns = [\"peakwavs_max\"]  # list of names of the columns containing targets\n",
    "df_input = pd.read_csv(input_path)\n",
    "smiss = df_input.loc[:, smiles_columns].values\n",
    "ys = df_input.loc[:, target_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [[MoleculeDatapoint.from_smi(smis[0], y) for smis, y in zip(smiss, ys)]]\n",
    "all_data += [[ComponentDatapoint.from_smi(smis[0], w_fp=0.1) for smis in smiss]]\n",
    "all_data += [[ComponentDatapoint.from_smi(smis[1], w_fp=0.9) for smis in smiss]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Mixsolv-QM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemprop_dir = Path.cwd().parent\n",
    "input_path = (\n",
    "    chemprop_dir / \"examples\" / \"data\" / \"MixSolvGH-QM.csv\"\n",
    ")  # path to your data .csv file containing SMILES strings and target values\n",
    "smiles_columns = [\"inchi_solute\", \"inchi_solvent1\", \"inchi_solvent2\"]  # name of the column containing SMILES strings\n",
    "frac_columns = [\"frac_solvent1\"]\n",
    "target_columns = [\"Gsolv (kcal/mol)\"]  # list of names of the columns containing targets\n",
    "df_input_sample = pd.read_csv(input_path, sep=\",\").iloc[:100000]\n",
    "smiss = df_input_sample.loc[:, smiles_columns].apply(lambda col: col.apply(lambda x: Chem.MolToSmiles(Chem.MolFromInchi(x)) if x is not np.nan else None)).values\n",
    "fracs = df_input_sample.loc[:, frac_columns]\n",
    "fracs[\"frac_solvent1\"] = fracs[\"frac_solvent1\"].fillna(1.0) # fill in empty molfracs columns with just one component\n",
    "fracs = fracs.values\n",
    "ys = df_input_sample.loc[:, target_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [[MoleculeDatapoint.from_smi(smis[0], y) for smis, y in zip(smiss, ys)]]\n",
    "all_data += [[ComponentDatapoint.from_smi(smis[1], w_fp=f[0]) for smis, f in zip(smiss, fracs)]]\n",
    "all_data += [[ComponentDatapoint.from_smi(smis[2], w_fp=1-f[0]) if smis[2] else ComponentDatapoint(None) for smis, f in zip(smiss, fracs)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue from here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_to_split_by = 0  # index of the component to use for structure based splits\n",
    "mols = [d.mol for d in all_data[component_to_split_by]]\n",
    "train_indices, val_indices, test_indices = make_split_indices(mols, \"random\", (0.8, 0.1, 0.1))\n",
    "train_data, val_data, test_data = split_data_by_indices(\n",
    "    all_data, train_indices, val_indices, test_indices\n",
    ")\n",
    "train_data = train_data[0]\n",
    "val_data = val_data[0]\n",
    "test_data = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = [\n",
    "    MoleculeDataset(train_data[0]),\n",
    "    ComponentDataset(train_data[1]),\n",
    "    ComponentDataset(train_data[2]),\n",
    "]\n",
    "val_datasets = [\n",
    "    MoleculeDataset(val_data[0]),\n",
    "    ComponentDataset(val_data[1]),\n",
    "    ComponentDataset(val_data[2]),\n",
    "]\n",
    "test_datasets = [\n",
    "    MoleculeDataset(test_data[0]),\n",
    "    ComponentDataset(test_data[1]),\n",
    "    ComponentDataset(test_data[2]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mcdset = MixtureDataset(train_datasets)\n",
    "scaler = train_mcdset.normalize_targets()\n",
    "val_mcdset = MixtureDataset(val_datasets)\n",
    "val_mcdset.normalize_targets(scaler)\n",
    "test_mcdset = MixtureDataset(test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_mcdset, batch_size=10, shuffle=True, collate_fn=collate_mixture)\n",
    "val_loader = DataLoader(val_mcdset, batch_size=10, shuffle=False, collate_fn=collate_mixture)\n",
    "test_loader = DataLoader(test_mcdset, batch_size=10, shuffle=False, collate_fn=collate_mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmp = MulticomponentMessagePassing(blocks=[BondMessagePassing(), BondMessagePassing()], groups=[[0], [1, 2]], shared=False)\n",
    "mixmp = MixtureMessagePassing(d_v=mcmp.blocks[0].output_dim, d_h=mcmp.blocks[0].output_dim, depth=1)\n",
    "\n",
    "graph_agg = MeanAggregation()\n",
    "name_agg = \"weightedsum\"\n",
    "\n",
    "match name_agg:\n",
    "    case \"weightedsum\":\n",
    "        mixagg = WeightedSumAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case \"cat\":\n",
    "        mixagg = ConcatAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case \"deepsets\":\n",
    "        mixagg = DeepsetsAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case \"attentive\":\n",
    "        mixagg = AttentiveAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case \"set2set\":\n",
    "        mixagg = Set2SetAggregation(\n",
    "            graph_agg=graph_agg, groups=[[0], [1, 2]], fp_dims=[mcmp.blocks[0].output_dim] * 3, mixmp=mixmp,\n",
    "        )\n",
    "    case _:\n",
    "        raise ValueError(f\"MixtureAggregation {name_agg} not implemented yet.\")\n",
    "\n",
    "output_transform = UnscaleTransform.from_standard_scaler(scaler)\n",
    "ffn = RegressionFFN(input_dim=mixagg.output_dim, output_transform=output_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmpnn = MixtureMPNN(mcmp, mixagg, ffn)\n",
    "mcmpnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    logger=False,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    max_epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(mcmpnn, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.test(mcmpnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.predict(mcmpnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader.dataset.datasets[0].Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "from lightning import pytorch as pl\n",
    "from numpy.typing import ArrayLike\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torchmetrics\n",
    "from torchmetrics.functional.classification.hinge import (\n",
    "    _multiclass_hinge_loss_tensor_validation,\n",
    "    _multiclass_confusion_matrix_format,\n",
    ")\n",
    "from torchmetrics.utilities.data import to_onehot\n",
    "\n",
    "from chemprop import data, featurizers, models, nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Chemprop, loss functions and metrics are both instances of `chemprop.nn.metrics.ChempropMetric`, which inherits from `torchmetrics.Metric`. This notebook shows how to adapt loss functions and metrics from `torchmetrics` to work in Chemprop. Custom loss functions and metrics that are not available in `torchmetrics` can be created following the instructions provided on the `torchmetrics` website and then adapted to Chemprop by following the example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemprop_dir = Path.cwd().parent\n",
    "input_path = chemprop_dir / \"tests\" / \"data\" / \"classification\" / \"mol_multiclass.csv\"\n",
    "df_input = pd.read_csv(input_path)\n",
    "smis = df_input.loc[:, \"smiles\"].values\n",
    "ys = df_input.loc[:, [\"activity\"]].values\n",
    "all_data = [data.MoleculeDatapoint.from_smi(smi, y) for smi, y in zip(smis, ys)]\n",
    "train_indices, val_indices, test_indices = data.make_split_indices(all_data, \"random\", (0.8, 0.1, 0.1))\n",
    "train_data, val_data, test_data = data.split_data_by_indices(\n",
    "    all_data, train_indices, val_indices, test_indices\n",
    ")\n",
    "train_dset = data.MoleculeDataset(train_data)\n",
    "val_dset = data.MoleculeDataset(val_data)\n",
    "test_dset = data.MoleculeDataset(test_data)\n",
    "train_loader = data.build_dataloader(train_dset)\n",
    "val_loader = data.build_dataloader(val_dset, shuffle=False)\n",
    "test_loader = data.build_dataloader(test_dset, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ChempropMetric`s optionally expand the capabilities of `torchmetrics.Metric`s by allowing for weighting tasks and data points in the loss function. Additionally, targets can be masked out to not be used in the calculation or marked as one sided (meaning predictions greater than or, alternatively, less than target won't be penalized). \n",
    "\n",
    "The `__init__` method of custom loss functions and metrics should accept `task_weights` as an argument. They may not be used in the actual calculation. The `update` should accept `preds, target, mask, weights, lt_mask, gt_mask`, even if mask, weights, lt_mask, and gt_mask are not used in the calculation. Greater than and less than masks only apply to regression tasks for example. \n",
    "\n",
    "`ChempropMetrics`s should also have an alias property. This is used when logging the metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChempropMulticlassHingeLoss(torchmetrics.classification.MulticlassHingeLoss):\n",
    "    def __init__(self, task_weights: ArrayLike = 1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.task_weights = torch.as_tensor(task_weights, dtype=torch.float).view(1, -1)\n",
    "        if (self.task_weights != 1.0).any():\n",
    "            warnings.warn(\"task_weights were provided but are ignored by metric \"\n",
    "                          f\"{self.__class__.__name__}. Got {task_weights}\")\n",
    "\n",
    "    def update(self, preds: Tensor, targets: Tensor, mask: Tensor | None = None, *args, **kwargs):\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(targets, dtype=torch.bool)\n",
    "\n",
    "        super().update(preds[mask], targets[mask].long())\n",
    "\n",
    "    @property\n",
    "    def alias(self) -> str:\n",
    "        return \"hinge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChempropMulticlassAUROC(torchmetrics.classification.MulticlassAUROC):\n",
    "    def __init__(self, task_weights: ArrayLike = 1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.task_weights = torch.as_tensor(task_weights, dtype=torch.float).view(1, -1)\n",
    "        if (self.task_weights != 1.0).any():\n",
    "            warnings.warn(\"task_weights were provided but are ignored by metric \"\n",
    "                          f\"{self.__class__.__name__}. Got {task_weights}\")\n",
    "\n",
    "    def update(self, preds: Tensor, targets: Tensor, mask: Tensor | None = None, *args, **kwargs):\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(targets, dtype=torch.bool)\n",
    "\n",
    "        super().update(preds[mask], targets[mask].long())\n",
    "\n",
    "    @property\n",
    "    def alias(self) -> str:\n",
    "        return \"multiclass_auroc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supply the custom loss function and metric to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = max(ys).item() + 1\n",
    "\n",
    "loss_function = ChempropMulticlassHingeLoss(num_classes = n_classes)\n",
    "ffn = nn.MulticlassClassificationFFN(n_classes=n_classes, criterion=loss_function)\n",
    "\n",
    "metrics = [ChempropMulticlassAUROC(num_classes=n_classes)]\n",
    "\n",
    "model = models.MPNN(nn.BondMessagePassing(), nn.NormAggregation(), ffn, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/knathan/anaconda3/envs/chemprop/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/knathan/anaconda3/envs/chemprop/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name            | Type                        | Params | Mode \n",
      "------------------------------------------------------------------------\n",
      "0 | message_passing | BondMessagePassing          | 227 K  | train\n",
      "1 | agg             | NormAggregation             | 0      | train\n",
      "2 | bn              | BatchNorm1d                 | 600    | train\n",
      "3 | predictor       | MulticlassClassificationFFN | 91.2 K | train\n",
      "4 | X_d_transform   | Identity                    | 0      | train\n",
      "5 | metrics         | ModuleList                  | 0      | train\n",
      "------------------------------------------------------------------------\n",
      "319 K     Trainable params\n",
      "0         Non-trainable params\n",
      "319 K     Total params\n",
      "1.278     Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/knathan/anaconda3/envs/chemprop/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:363: Skipping 'metrics' parameter because it is not possible to safely dump to YAML.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knathan/anaconda3/envs/chemprop/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knathan/anaconda3/envs/chemprop/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 7/7 [00:01<00:00,  4.31it/s, v_num=23, train_loss_step=0.336, val_loss=0.930, train_loss_epoch=0.516]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 7/7 [00:01<00:00,  4.21it/s, v_num=23, train_loss_step=0.336, val_loss=0.930, train_loss_epoch=0.516]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knathan/anaconda3/envs/chemprop/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 19.18it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knathan/anaconda3/envs/chemprop/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   test/multiclass_auroc   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m  test/multiclass_auroc  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/multiclass_auroc': 0.0}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=2)\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
